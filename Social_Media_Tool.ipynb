{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ef6c6e-b534-4f4c-814f-e6f5114b4187",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Roberto Di Via 4486648 - NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f544ffc-1390-4056-b2ce-d0e9da4f0cac",
   "metadata": {},
   "source": [
    "# Social Media Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c807eee-6edf-446c-8302-da94aa9e70bf",
   "metadata": {},
   "source": [
    "In this project i'll implement a social media monitor that tracks topics or trends from social media or blogs. This project can help businesses or individuals stay up-to-date with the latest developments and discussions related to their areas of interest.\n",
    "\n",
    "To implement this project, I'll follow these steps:\n",
    "\n",
    "- **1. Data Collection:** Gather data from various sources like news websites, blogs, and social media using APIs or web scraping techniques or RSS feed. In this case I'll use the 20newsgroups dataset from Sklearn that comprises around 18000 newsgroups posts on 20 topics.\n",
    "- **2. Text Preprocessing:** Clean and normalize the text data using stopword removal, stemming and lemmatization.\n",
    "- **3. Topic Modeling:** Employ topic modeling techniques like Latent Dirichlet Allocation (LDA) to identify the main topics or themes present in the collected data. This will help filter relevant content based on the topics of interest.\n",
    "- **4. Sentiment Analysis:** Determine the sentiment of the content (positive, negative, or neutral) using a rule-based approach like VADER sentiment analyzer.\n",
    "- **5. Summarization:** Generate summaries of the relevant content using extractive summarization based on word frequencies, so that users can quickly grasp the main points without reading the entire text.\n",
    "- **6. Visualization and Reporting:** Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2a2ee-b7b3-4d83-92f1-6eada6776a96",
   "metadata": {},
   "source": [
    "# **0. Import libraries:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbe65e-86d5-4379-9ddd-f036ba5453bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install pyLDAvis\n",
    "%pip install vaderSentiment\n",
    "%pip install wordcloud\n",
    "%pip install matplotlib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc297b-6397-49e9-b2c7-4e383dfa9c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# --------------- Dataset ------------- #\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# --------------- Pre-Processing -------- #\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# --------------- LDA Model ---------- #\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "# --------------- Sentiment Analysis --------- #\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# --------------- \n",
    "\n",
    "# --------------- Visualize and Report ----------- #\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e656e-b9db-4705-84ea-edf8844b994c",
   "metadata": {},
   "source": [
    "# **1. Data Collection:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d688647-8e11-458e-9898-c9b55b3a8cf0",
   "metadata": {},
   "source": [
    "Data can be collected from various sources like news websites, blogs, and social media using APIs or web scraping techniques or RSS feed. In this case I use the 20newsgroups dataset from Sklearn, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "More informations can be found at this [link](http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08affa98-8e7d-4386-85e9-14cad963c1e0",
   "metadata": {},
   "source": [
    "**Exploring the 20newsgroups dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198dca9c-e1ff-4ac7-ba1e-edfbc9eb9d52",
   "metadata": {},
   "source": [
    "First, I remove the headers, footers and quotes from the texts. The difference between the original text and the clean text that I'll use for my experiments are showed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616e836-2491-44a4-af13-6d7780cf87a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_text = fetch_20newsgroups(subset='test', categories=['sci.space'])\n",
    "print(f\"Original text:\\n{example_text.data[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4e757-b1ca-4354-9fab-7a93da506d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_example_text = fetch_20newsgroups(subset='test', categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "print(f\"Clean text:\\n{clean_example_text.data[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a874892-9cf4-4936-a6a0-1a224f162b95",
   "metadata": {},
   "source": [
    "Then, I check that the training topics are balanced, so there isn't bias during the training of the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98273d9-fff1-473e-ad37-686a1a7b6579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify train dataset is balanced\n",
    "motorcycles_train = fetch_20newsgroups(subset='train', categories=['rec.motorcycles'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Motorcycles dataset size: \", len(motorcycles_train.data))\n",
    "\n",
    "hardware_train = fetch_20newsgroups(subset='train', categories=['comp.sys.ibm.pc.hardware'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Hardware dataset size: \", len(hardware_train.data))\n",
    "\n",
    "graphics_train = fetch_20newsgroups(subset='train', categories=['comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Graphics dataset size: \", len(graphics_train.data))\n",
    "\n",
    "med_train = fetch_20newsgroups(subset='train', categories=['sci.med'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Med dataset size: \", len(med_train.data))\n",
    "\n",
    "space_train = fetch_20newsgroups(subset='train', categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Space dataset size: \", len(space_train.data))\n",
    "\n",
    "guns_train = fetch_20newsgroups(subset='train', categories=['talk.politics.guns'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Guns dataset size: \", len(guns_train.data))\n",
    "\n",
    "crypt_train = fetch_20newsgroups(subset='train', categories=['sci.crypt'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Crypt dataset size: \", len(crypt_train.data))\n",
    "\n",
    "forsale = fetch_20newsgroups(subset='train', categories=['misc.forsale'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Forsale dataset size: \", len(forsale.data))\n",
    "\n",
    "christian = fetch_20newsgroups(subset='train', categories=['soc.religion.christian'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Christian dataset size: \", len(christian.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644a0ff-7883-4584-8e7a-024f22d79e7d",
   "metadata": {},
   "source": [
    "**Train dataset creation**\n",
    "\n",
    "I create the train dataset. In this case I consider only 4 training topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313f6d9-7343-4879-81a0-9af81cb22650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train dataset with selected categories\n",
    "train_categories = ['talk.religion.misc', 'rec.autos', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=train_categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(newsgroups_train.target_names)\n",
    "\n",
    "n=30\n",
    "print(f\"Topics of first {n} texts: {newsgroups_train.target[:n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59960efa-a459-49d0-bed2-6d91702e2f4d",
   "metadata": {},
   "source": [
    "**Test dataset creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3904b5-5d37-4e00-9ab8-f1172e5f0426",
   "metadata": {},
   "source": [
    "For the test set, I create a custom dataset composed by 100 documents and 2 topics: 70% rec.autos and 30% sci.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61170b2a-afdf-477f-a3f2-da74523e9b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create test dataset with 100 samples: 70% guns and 20% space and 10% med\n",
    "n_test_docs = 100\n",
    "n_docs_70 = int(n_test_docs * 0.7)\n",
    "n_docs_30 = n_test_docs - n_docs_70\n",
    "\n",
    "# Fetch data for each category\n",
    "test_70 = fetch_20newsgroups(subset='test', categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "test_30 = fetch_20newsgroups(subset='test', categories=['comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Topic 1 size: \", len(test_70.data))\n",
    "print(\"Topic 2 size: \", len(test_30.data))\n",
    "\n",
    "# Randomly select the desired number of documents from each category\n",
    "docs70_indices = np.random.choice(len(test_70.data), n_docs_70, replace=False)\n",
    "docs30_indices = np.random.choice(len(test_30.data), n_docs_30, replace=False)\n",
    "\n",
    "# Create the test dataset\n",
    "test_data = [test_70.data[i] for i in docs70_indices] + [test_30.data[i] for i in docs30_indices]\n",
    "\n",
    "\"\"\"\n",
    "test_target = np.concatenate((guns_test.target[docs70_indices], space_test.target[docs20_indices], med_test.target[docs10_indices]))\n",
    "\n",
    "# Create the newsgroups_test dataset\n",
    "newsgroups_test = {\n",
    "    'data': test_data,\n",
    "    'target': test_target,\n",
    "    'target_names': ['talk.politics.guns', 'sci.space', 'sci.med']\n",
    "}\n",
    "\"\"\"\n",
    "#print(\"Newsgroups test dataset size: \", len(newsgroups_test['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09b4ca-6e40-42ea-8651-b096592ff444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_data[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e6e70-ea46-437a-933b-4c36c5603d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train dataset size: \", len(newsgroups_train.data))\n",
    "print(\"Train topics are:\\n\",newsgroups_train.target_names)\n",
    "\n",
    "print(\"\\nTest dataset size: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51b685-8c16-42eb-b823-7824e7055ad5",
   "metadata": {},
   "source": [
    "# **2. Text Preprocessing:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5ac49-7f54-4e52-8939-7a1f71fa0df0",
   "metadata": {},
   "source": [
    "I clean and normalize the text data using tokenization, stopword removal, and stemming/lemmatization. I use the `nltk` library for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55868e82-bb4d-4517-929d-b4303ef2372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    # Remove punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def remove_special_chars(text):\n",
    "        # Replace anything that is not an alphanumeric character or a space with an empty string\n",
    "        return re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n",
    "\n",
    "    def tokenize(text):\n",
    "        text = remove_special_chars(text) # Remove special characters before tokenization\n",
    "        return [word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    def lemmatize(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_data = [tokenize(text) for text in data]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_data = [[stemmer.stem(token) for token in text] for text in tokenized_data]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatized_data = [lemmatize(text) for text in tokenized_data]\n",
    "\n",
    "    return tokenized_data, stemmed_data, lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d0439-7fdd-4fe0-94a5-5c5187f04e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess train dataset\n",
    "tokenized_train_data, stemmed_train_data, lemmatized_train_data = preprocess_text(newsgroups_train.data)\n",
    "\n",
    "# Preprocess test dataset\n",
    "tokenized_test_data, stemmed_test_data, lemmatized_test_data = preprocess_text(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053fc7b-6a85-4cf7-8d3f-9c4bf489c239",
   "metadata": {},
   "source": [
    "Let's visualize the difference between a tokenized text, stemmed text and a lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c4c1d-73e0-4486-8927-76be4e64fd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenized_train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bf5d2-7ee1-46f8-997f-3b0c958f0549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(stemmed_train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae15115-95df-4a7b-9df3-57bae75167db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lemmatized_train_data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19967faa-f869-497b-beea-bdf329f79f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **3. Topic Modeling:** \n",
    "Apply Latent Dirichlet Allocation (LDA) to identify the main topics in the collected data. In this part I'll use a scratch implementation and compare it with `gensim` library version. To evaluate both models on the testing set I compute the coherence scores.\n",
    "\n",
    "Remember, for topic modeling, you can train your model on any similar corpus of text documents. It doesn't necessarily have to contain the same topics as your unseen documents but having some overlap would likely improve performance. For example, if you're looking to categorize social media posts from a specific platform or about a specific subject, you would ideally use a training set gathered from the same or similar platform/subject.\n",
    "\n",
    "However, if you want to train an LDA model on the specific topics you mentioned, you would need a dataset that contains a substantial number of documents related to these topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af215bc7-7375-4f5e-8f35-bb80c39ad4ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf18aa-5e2f-445f-89a9-bc8d498bbfbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Latent Dirichlet Allocation (LDA) is a generative probabilistic model used in topic modeling. It is a statistical model that allows us to discover latent topics within a collection of documents. LDA assumes that each document in the collection is a mixture of various topics, and each topic is a distribution over words.\n",
    "\n",
    "It's an unsupervised learning method, meaning that it generates a probabilistic model to identify groups of topics without the need for known class labels. It uses only the distribution of words to mathematically model topic.\n",
    "\n",
    "Here's a step-by-step explanation of how LDA works:\n",
    "\n",
    "1. **Initialization**: Choose the number of topics K to extract from the document collection and randomly assign each word in each document to one of the K topics.\n",
    "\n",
    "2. **Iteration**: Iterate through each word in each document and reassign the word to a topic based on: the proportion of words in the document that belong to the topic, and the proportion of occurrences of the word across all documents that belong to the topic.\n",
    "   - For each document d:\n",
    "     - For each word w in document d:\n",
    "       - Calculate two probabilities:\n",
    "         - P(topic t | document d): Proportion of words in document d that are currently assigned to topic t.\n",
    "         - P(word w | topic t): Proportion of assignments to topic t over all documents that come from word w.\n",
    "       - Reassign word w to a new topic based on the probabilities calculated above.\n",
    "   \n",
    "   - Repeat the above step for a fixed number of iterations or until convergence.\n",
    "\n",
    "3. **Output**: Repeat step 2 for a certain number of iterations or until convergence. LDA provides two main outputs:\n",
    "   - The distribution of topics in each document.\n",
    "   - The distribution of words in each topic.\n",
    "\n",
    "These distributions can be used to interpret the topics and analyze the relationships between documents and topics.\n",
    "\n",
    "\n",
    "\n",
    "LDA assumes that documents are generated in the following way:\n",
    "- Choose the number of words in the document from a Poisson distribution.\n",
    "- Choose a topic mixture for the document from a Dirichlet distribution.\n",
    "- For each word in the document:\n",
    "  - Choose a topic from the topic mixture.\n",
    "  - Choose a word from the topic's word distribution.\n",
    "\n",
    "LDA is widely used in natural language processing and text mining tasks, such as document clustering, document classification, and information retrieval. It helps uncover the underlying themes or topics in a collection of documents, making it easier to analyze and organize large amounts of textual data[1].\n",
    "\n",
    "Please note that the search results provided additional papers and applications related to LDA, which you can explore for more specific information and use cases.\n",
    "\n",
    "Citations:\n",
    "[1] https://www.semanticscholar.org/paper/b98a4076b48552691bb99290106a378e483cdfca\n",
    "[2] https://www.semanticscholar.org/paper/03ba268430128916e195e8d1a88c761f3c9d7578\n",
    "[3] https://arxiv.org/abs/1309.3421\n",
    "[4] https://www.semanticscholar.org/paper/c80db2cd1b127ec86060ad018c04cd0c48075ae3\n",
    "[5] https://www.semanticscholar.org/paper/1713b2a9291d76c02feb49376422d800d5e44888\n",
    "[6] https://www.semanticscholar.org/paper/59c902e7797889bad1f731205a409ade2913199a\n",
    "\n",
    "\n",
    "The documents can come from any domain as long as they contain text. For example, they could be customer reviews, news articles, research papers, social media posts, etc. The words in the documents are collected into n-grams (a contiguous sequence of n items from a given sample of text or speech) and used to create a dictionary. This dictionary is then used to train the LDA model. \n",
    "\n",
    "It's important to note that the text in the documents should be preprocessed before being used for training the LDA model. This preprocessing can include removing stop words (commonly used words such as 'the', 'a', 'an', 'in'), lowercasing all the words, and lemmatizing the words (reducing inflectional forms and sometimes derivationally related forms of a word to a common base form)\n",
    "\n",
    "When configuring the LDA model, some parameters that can be set include the rho parameter (a prior probability for the sparsity of topic distributions), the alpha parameter (a prior probability for the sparsity of per-document topic weights), the estimated number of documents, the size of the batch, the initial value of iteration used in learning update schedule, the power applied to the iteration during updates, and the number of passes over the data\n",
    "\n",
    "\n",
    "As a result of the training, each document will be represented as a combination of topics, and each topic will be represented as a distribution over words. This can be used to classify new documents, identify related terms, and create recommendations.\n",
    "\n",
    "![LDA](https://www.researchgate.net/profile/Diego-Buenano-Fernandez/publication/339368709/figure/fig1/AS:860489982689280@1582168207260/Schematic-of-LDA-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fe482-a1ed-47ce-a18d-e2f6807b3522",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55014f19-2030-4e8f-a44d-1eabff08bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(lemmatized_train_data)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "train_corpus = [dictionary.doc2bow(text) for text in lemmatized_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461a926-5734-49ed-bdd6-61d3e78b377f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# human-readable format of corpus (term-frequency)\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in train_corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9b0b9-728f-4d4d-8119-b17ccc993cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of topics to extract from documents\n",
    "num_topics = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29786fa-50a0-4530-9eea-7749b303490a",
   "metadata": {},
   "source": [
    "### Gensim version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2490f1e-7e06-4524-913a-a619e0ad125c",
   "metadata": {},
   "source": [
    "Let's build the topic model. I'll define 5 topics to start with. The hyperparameter alpha affects sparsity of the document-topic (theta) distributions, whose default value is 1. Similarly, the hyperparameter eta can also be specified, which affects the topic-word distribution's sparsity.\n",
    "\n",
    "https://www.kaggle.com/code/datajameson/topic-modelling-nlp-amazon-reviews-bbc-news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1cabda-e98d-4820-8805-392a792698c0",
   "metadata": {},
   "source": [
    "**Training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17fee-75ef-4066-96aa-6429ed741ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the LDA model using the processed training data\n",
    "lda_gensim = LdaModel(corpus=train_corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3f129-4ce5-4645-bf32-7f41117dfd56",
   "metadata": {},
   "source": [
    "We explores the trained topics with their main words and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b505b-303c-43d0-bb3e-741b1d90e580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping between topic number and category name\n",
    "def topic_category_mapping(topic_id):\n",
    "    if topic_id == 0:\n",
    "        return 'Car'\n",
    "    elif topic_id == 1:\n",
    "        return \"Graphics\"\n",
    "    elif topic_id == 2:\n",
    "        return \"Space\"\n",
    "    elif topic_id == 3:\n",
    "        return 'Religion'\n",
    "    else:\n",
    "        return f'Unknown Category {topic_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cab14b-21bb-4c13-b7b1-2132cbede10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics and associated category names\n",
    "for topic_num, topic in lda_gensim.show_topics(num_topics=num_topics, formatted=False):\n",
    "    topic_words = [word for word, _ in topic]\n",
    "    category_name = topic_category_mapping(topic_num)\n",
    "    print(f\"Topic {topic_num} ({category_name}) | Words: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a8d77-d81f-4d60-88df-8620c86b7efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_gensim, train_corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_vis.html')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "469e83b6-0684-4bb8-bfa7-f18501a5d671",
   "metadata": {},
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f19e10-8821-49a3-83fb-3de2698a5724",
   "metadata": {},
   "source": [
    "**Test phase**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2cc01-19a6-45e8-94e1-3338e4a33319",
   "metadata": {},
   "source": [
    "Get the topics distributions for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ee1c3-7617-4c1f-8edb-2ed7f7ff7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic_distributions = [lda_gensim.get_document_topics(dictionary.doc2bow(text)) for text in lemmatized_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3d16a-edd6-4548-9b6c-032c9c2c6960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(test_topic_distributions):\n",
    "    # Sort the topic distribution by probability in descending order\n",
    "    sorted_topic_dist = sorted(topic_dist, key=lambda x: -x[1])\n",
    "    # Create a list to store the formatted topics\n",
    "    formatted_topics = []\n",
    "\n",
    "    # Format and store each topic\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        # Associate label to the topic\n",
    "        category_name = topic_category_mapping(topic_id)\n",
    "\n",
    "        formatted_topic = f\"[{topic_id}] {category_name} {probability:.2f}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "\n",
    "    # Join the formatted topics into a string\n",
    "    formatted_topics_str = \" - \".join(formatted_topics)\n",
    "\n",
    "    print(f\"Document {i + 1} topics : {formatted_topics_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af18d0f-8342-4493-bd80-c96a57a9ca6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scratch version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff373130-89e5-4e0c-9592-7bc6d4a2cd7a",
   "metadata": {},
   "source": [
    "I implement the LDA model from scratch. The input corpus is in the Gensim bag-of-words format, which is a list of tuples (word index, word count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeae21-e534-40f0-9b07-8f14d2c9280c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, num_topics, num_iterations=30, id2word=None, alpha=0.1, beta=0.1):\n",
    "        self.num_topics = num_topics\n",
    "        self.num_iterations = num_iterations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.topic_word_counts = None\n",
    "        self.doc_topic_counts = None\n",
    "        self.id2word = id2word\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.num_words = max([word_idx for doc in corpus for word_idx, _ in doc]) + 1\n",
    "        self.initialize()\n",
    "        self.sample_topics()\n",
    "\n",
    "    def initialize(self):\n",
    "        # Initialize topic assignments randomly\n",
    "        self.topic_assignments = [[random.randint(0, self.num_topics - 1) for _ in doc] for doc in self.corpus]\n",
    "\n",
    "        # Initialize topic-word and document-topic count matrices\n",
    "        self.topic_word_counts = np.zeros((self.num_topics, self.num_words))\n",
    "        self.doc_topic_counts = np.zeros((len(self.corpus), self.num_topics))\n",
    "\n",
    "        # Count initial topic assignments\n",
    "        for doc_idx, doc in enumerate(self.corpus):\n",
    "            for word_idx, (word, count) in enumerate(doc):\n",
    "                topic = self.topic_assignments[doc_idx][word_idx]\n",
    "                self.topic_word_counts[topic][word] += count\n",
    "                self.doc_topic_counts[doc_idx][topic] += count\n",
    "\n",
    "    def sample_topics(self):\n",
    "        # Perform Gibbs sampling\n",
    "        for it in range(self.num_iterations):\n",
    "            doc_topic_sums = self.doc_topic_counts.sum(axis=1)\n",
    "            topic_word_sums = self.topic_word_counts.sum(axis=1)\n",
    "            \n",
    "            for doc_idx, doc in enumerate(self.corpus):\n",
    "                for word_idx, (word, count) in enumerate(doc):\n",
    "                    # Remove current topic assignment\n",
    "                    old_topic = self.topic_assignments[doc_idx][word_idx]\n",
    "                    self.topic_word_counts[old_topic][word] -= count\n",
    "                    self.doc_topic_counts[doc_idx][old_topic] -= count\n",
    "                    doc_topic_sums[doc_idx] -= count\n",
    "                    topic_word_sums[old_topic] -= count\n",
    "\n",
    "                    # Compute probabilities for each topic (conditional distribution for the word)\n",
    "                    p_topic_given_doc = (self.doc_topic_counts[doc_idx, :] + self.alpha) / (doc_topic_sums[doc_idx] + self.num_topics * self.alpha)\n",
    "                    p_word_given_topic = (self.topic_word_counts[:, word] + self.beta) / (topic_word_sums + self.num_words * self.beta)\n",
    "                    probabilities = p_topic_given_doc * p_word_given_topic\n",
    "\n",
    "                    # Normalize probabilities\n",
    "                    probabilities /= probabilities.sum()\n",
    "\n",
    "                    # Sample a new topic assignment\n",
    "                    new_topic = np.random.choice(self.num_topics, p=probabilities)\n",
    "                    self.topic_assignments[doc_idx][word_idx] = new_topic\n",
    "\n",
    "                    # Update counts for new topic assignment\n",
    "                    self.topic_word_counts[new_topic][word] += count\n",
    "                    self.doc_topic_counts[doc_idx][new_topic] += count\n",
    "                    doc_topic_sums[doc_idx] += count\n",
    "                    topic_word_sums[new_topic] += count\n",
    "\n",
    "            #print(f\"Iteration {it}\")\n",
    "    \n",
    "    # Method used in the CoherenceModel \n",
    "    def get_topics(self):\n",
    "        topics = np.zeros((self.num_topics, self.num_words))\n",
    "        for topic_idx in range(self.num_topics):\n",
    "            topic_word_counts = self.topic_word_counts[topic_idx, :]\n",
    "            topic_word_probs = topic_word_counts / topic_word_counts.sum()\n",
    "            topics[topic_idx, :] = topic_word_probs\n",
    "        return topics  \n",
    "        \n",
    "    def show_topics(self, num_topics=None, num_words=10):\n",
    "        if num_topics is None:\n",
    "            num_topics = self.num_topics\n",
    "\n",
    "        topics = []\n",
    "        for topic_idx in range(num_topics):\n",
    "            topic_word_probs = self.topic_word_counts[topic_idx, :]\n",
    "            topic_word_probs /= topic_word_probs.sum()\n",
    "            top_word_indices = np.argsort(topic_word_probs)[-num_words:]\n",
    "            topic_words = [(self.id2word[word_idx], topic_word_probs[word_idx]) for word_idx in top_word_indices]\n",
    "            topics.append((topic_idx, topic_words))\n",
    "        return topics\n",
    "    \n",
    "    # Takes as input a specific topic id\n",
    "    def show_topic(self, topic_id, num_words=10):\n",
    "        topic_word_probs = self.topic_word_counts[topic_id, :]\n",
    "        topic_word_probs /= topic_word_probs.sum()\n",
    "        top_word_indices = np.argsort(topic_word_probs)[-num_words:]\n",
    "        topic_words = [(self.id2word[word_idx], np.round(topic_word_probs[word_idx], 9)) for word_idx in top_word_indices]\n",
    "        return topic_words\n",
    "    \n",
    "    def get_document_topics(self, unseen_document):\n",
    "        unseen_corpus = [unseen_document]\n",
    "        inferred_topics = self.inference(unseen_corpus)\n",
    "        return inferred_topics[0]\n",
    "    \n",
    "    def inference(self, unseen_corpus):\n",
    "        inferred_topics = []\n",
    "        for doc in unseen_corpus:\n",
    "            doc_topic_counts = np.zeros(self.num_topics)\n",
    "            for word_idx, (word, count) in enumerate(doc):\n",
    "                p_topic_given_doc = (doc_topic_counts + self.alpha) / (doc_topic_counts.sum() + self.num_topics * self.alpha)\n",
    "                p_word_given_topic = (self.topic_word_counts[:, word] + self.beta) / (self.topic_word_counts.sum(axis=1) + self.num_words * self.beta)\n",
    "                probabilities = p_topic_given_doc * p_word_given_topic\n",
    "\n",
    "                # Normalize probabilities\n",
    "                probabilities /= probabilities.sum()\n",
    "\n",
    "                inferred_topic = np.random.choice(self.num_topics, p=probabilities)\n",
    "                doc_topic_counts[inferred_topic] += count\n",
    "\n",
    "            # Normalize topic probabilities, adding a small constant to avoid division by zero\n",
    "            doc_topic_probs = doc_topic_counts / (doc_topic_counts.sum() + 1e-10)\n",
    "\n",
    "            # Filter out probabilities below the threshold\n",
    "            #doc_topic_probs = doc_topic_probs[doc_topic_probs > 0.2]\n",
    "    \n",
    "            # Convert the inferred topic counts to a list of tuples (topic_id, probability)\n",
    "            topic_distribution = [(topic_id, prob) for topic_id, prob in enumerate(doc_topic_probs)]\n",
    "            inferred_topics.append(topic_distribution)\n",
    "\n",
    "        return inferred_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f7293-47fc-4e6b-aa3b-4a246b25cbcc",
   "metadata": {},
   "source": [
    "**Training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7e704-1a8b-4acb-8def-bad3c756e287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the LDA model using the processed training data\n",
    "lda_scratch = LDA(num_topics=num_topics, id2word=dictionary, num_iterations=30)\n",
    "lda_scratch.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c266911-7153-477e-af14-9683b9261ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping between topic number and category name\n",
    "def topic_category_mapping_scratch(topic_id):\n",
    "    if topic_id == 0:\n",
    "        return 'Car'\n",
    "    elif topic_id == 1:\n",
    "        return 'Graphics'\n",
    "    elif topic_id == 2:\n",
    "        return 'Space'\n",
    "    elif topic_id == 3:\n",
    "        return 'Religion'\n",
    "    else:\n",
    "        return f'Unknown Category {topic_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa375a-3c42-43b6-aa98-fc20fad3ec49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print topics and associated category names\n",
    "for topic_num, topic in lda_scratch.show_topics(num_topics=num_topics, num_words=15):\n",
    "    topic_words = [word for word, _ in topic]\n",
    "    category_name = topic_category_mapping(topic_num)\n",
    "    print(f\"Topic {topic_num} ({category_name}) | Words: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bea118-6f3e-402c-9054-50ee1a1d72d2",
   "metadata": {},
   "source": [
    "**Testing phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e7675-7a67-4be6-9cc8-b6e8c689ef2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_topic_distributions = [lda_scratch.get_document_topics(dictionary.doc2bow(text)) for text in lemmatized_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc907c2e-e533-46d4-baa1-acc43558e722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(test_topic_distributions):\n",
    "    # Sort the topic distribution by probability in descending order\n",
    "    sorted_topic_dist = sorted(topic_dist, key=lambda x: -x[1])\n",
    "    # Create a list to store the formatted topics\n",
    "    formatted_topics = []\n",
    "\n",
    "    # Format and store each topic\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        # Associate label to the topic\n",
    "        category_name = topic_category_mapping_scratch(topic_id)\n",
    "\n",
    "        formatted_topic = f\"[{topic_id}] {category_name} {probability:.2f}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "\n",
    "    # Join the formatted topics into a string\n",
    "    formatted_topics_str = \" - \".join(formatted_topics)\n",
    "\n",
    "    print(f\"Document {i + 1} topics : {formatted_topics_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526945e2-1faf-4d2d-9142-5ec502c79cb6",
   "metadata": {},
   "source": [
    "### Coherence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb54ca-45d0-48b5-9ce2-0a181a976cd5",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/understanding-topic-coherence-measures-4aa41339634c\n",
    "\n",
    "The [CoherenceModel](https://radimrehurek.com/gensim/models/coherencemodel.html) class in the gensim library is used for evaluating the coherence of topic models. It provides different measures for computing the coherence score. Here are the different measures used in the computation of coherence score for the LDA model:\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "- c_v: This measure calculates the coherence based on the pairwise word-similarity scores. It considers the co-occurrence of words within a sliding window in the corpus.\n",
    "- u_mass: This measure calculates the coherence based on the document co-occurrence statistics. It uses the logarithm of the ratio of the probability of observing the words in a topic to the probability of observing the words in the entire corpus.\n",
    "- c_uci: This measure calculates the coherence based on the pointwise mutual information (PMI) of words in a topic. It considers the co-occurrence of words within a sliding window in the corpus and compares it to the expected co-occurrence under a random distribution.\n",
    "- c_npmi: This measure calculates the coherence based on the normalized pointwise mutual information (NPMI) of words in a topic. It normalizes the PMI score by taking into account the rarity of the words.\n",
    "\n",
    "These measures provide different perspectives on the coherence of topics in a topic model. The choice of measure depends on the specific requirements and characteristics of the corpus and the topic model being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c244fab-6f4a-4d48-be64-6d4c5854817f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_coherence_score(model, texts, dictionary, coherence='c_v'):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=coherence)\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c269ef8-404a-4c8f-b6a4-7f4e11dea340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Coherence score for Gensim LDA:\", compute_coherence_score(lda_gensim, tokenized_train_data, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e8145-469a-4c6e-b6af-1227f3e2214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coherence score for LDA from scratch:\", compute_coherence_score(lda_scratch, tokenized_train_data, dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d55440-bb7e-4c58-81c5-e462bce7e21c",
   "metadata": {},
   "source": [
    "### Number of topics Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90429746-5dff-44f5-a44b-b1706714a7c9",
   "metadata": {},
   "source": [
    "Performing hyperparameter tuning with the LDA (Latent Dirichlet Allocation) model is important because it allows us to determine the optimal number of topics for a given text corpus. The initial number of topics is typically unknown, and it is necessary to extract it using an unsupervised approach. Hyperparameter tuning helps us find the best number of topics by evaluating different models based on their coherence scores.\n",
    "\n",
    "Hyperparameter tuning involves systematically varying the number of topics and evaluating the resulting models. This process helps us avoid overfitting or underfitting the data. Overfitting occurs when the model is too complex and captures noise or irrelevant patterns, while underfitting occurs when the model is too simple and fails to capture the underlying structure of the data. By finding the optimal number of topics, we strike a balance between model complexity and interpretability.\n",
    "\n",
    "Determining the optimal number of topics is crucial for effective topic modeling. It ensures that the resulting topics are meaningful and representative of the underlying content in the text corpus. Without hyperparameter tuning, we may end up with suboptimal or uninformative topics that do not capture the true essence of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a2976-377f-42ae-ad4a-44e0cf536f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(corpus, texts, dictionary, num_topics_range):\n",
    "    coherence_scores_scratch = []\n",
    "    coherence_scores_gensim = []\n",
    "\n",
    "    n_iterations = 30\n",
    "    \n",
    "    for num_topics in num_topics_range:\n",
    "        # Train Scratch LDA model\n",
    "        lda_scratch = LDA(num_topics=num_topics, id2word=dictionary, num_iterations=n_iterations)\n",
    "        lda_scratch.fit(corpus)\n",
    "\n",
    "        # Train Gensim LDA model\n",
    "        lda_gensim = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=n_iterations)\n",
    "\n",
    "        # Compute coherence score for Scratch LDA model\n",
    "        coherence_score_scratch = compute_coherence_score(lda_scratch, texts, dictionary)\n",
    "        coherence_scores_scratch.append(coherence_score_scratch)\n",
    "\n",
    "        # Compute coherence score for Gensim LDA model\n",
    "        coherence_score_gensim = compute_coherence_score(lda_gensim, texts, dictionary)\n",
    "        coherence_scores_gensim.append(coherence_score_gensim)\n",
    "\n",
    "    # Find optimal number of topics based on coherence score\n",
    "    optimal_num_topics_scratch = num_topics_range[np.argmax(coherence_scores_scratch)]\n",
    "    optimal_num_topics_gensim = num_topics_range[np.argmax(coherence_scores_gensim)]\n",
    "    print(f\"Optimal number of topics (scratch model): {optimal_num_topics_scratch} | Coherence score: {coherence_scores_scratch[np.argmax(coherence_scores_scratch)]}\")\n",
    "    print(f\"Optimal number of topics (gensim model): {optimal_num_topics_gensim} | Coherence score: {coherence_scores_gensim[np.argmax(coherence_scores_gensim)]}\")\n",
    "\n",
    "    # Plot coherence scores\n",
    "    plt.plot(num_topics_range, coherence_scores_scratch, label='LDA Scratch')\n",
    "    plt.plot(num_topics_range, coherence_scores_gensim, label='LDA Gensim')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Coherence Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_num_topics_scratch, optimal_num_topics_gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacbc73-e8f5-46a0-ad67-448c2e93b529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_topics_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "optimal_num_topics_scratch, optimal_num_topics_gensim = hyperparameter_tuning(train_corpus, tokenized_train_data, dictionary, num_topics_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3afca-0d13-42e4-9f72-083a63dbf38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f186f08-494f-48a6-bae9-25f3227d6332",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **4. Sentiment Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93052d18-4644-4707-9cdb-e6d1fbfb8b52",
   "metadata": {},
   "source": [
    "Determine the sentiment of the content using the VADER sentiment analyzer from the `vaderSentiment`library.\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained sentiment analysis tool specifically designed for social media texts and doesn't require preprocessing like tokenization, stemming, or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996cfb1-d1fd-467e-909c-26e7c93ca8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to analyze sentiment using VADER\n",
    "def get_sentiment(text):\n",
    "     # Initialize VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Perform sentiment analysis on individual words\n",
    "    words = text.split()\n",
    "    words_sentiment_scores = [analyzer.polarity_scores(word) for word in words]\n",
    "\n",
    "    # Divide words into positive, negative, and neutral lists\n",
    "    positive_words = [word for word, score in zip(words, words_sentiment_scores) if score['compound'] > 0]\n",
    "    negative_words = [word for word, score in zip(words, words_sentiment_scores) if score['compound'] < 0]\n",
    "\n",
    "    return sentiment_scores, positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c134384-52d4-4cd5-9f16-471274289860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sentiments = [get_sentiment(text) for text in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64681b6-d2b8-45c3-a847-d77cbc338ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the sentiment scores for the first 10 test documents\n",
    "scores_list = []\n",
    "global_normalized_score = 0\n",
    "for i, (sentiment, pos_words, neg_words) in enumerate(test_sentiments):\n",
    "    scores_list.append(sentiment['compound'])\n",
    "    global_normalized_score += sentiment['compound']\n",
    "    print(f\"Document {i + 1}: {sentiment}\")\n",
    "    \n",
    "print(f\"\\nAverage sentiment: {np.mean(scores_list)}\")\n",
    "print(f\"\\nGlobal normalized sentiment: {global_normalized_score/len(test_sentiments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d695c42-a3fb-4286-80b4-11ed8777db26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebc9b40b-c510-49bd-b001-4b5bb262c9fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **5. Summarization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97070c92-b042-4a39-aaaf-a1d5e61c73bd",
   "metadata": {},
   "source": [
    "Generate summaries of the relevant content using extractive summarization based on word frequency. For this, I'll follow these steps:\n",
    "- 1. Split the text into sentences.\n",
    "- 2. Tokenize the sentences.\n",
    "- 3. Calculate the frequency of each word in the text.\n",
    "- 4. Assign a score to each sentence based on the frequency of the words in the sentence.\n",
    "- 5. Select the top N sentences with the highest scores as the summary.\n",
    "\n",
    "This is a simple implementation of extractive summarization without using any libraries. Note that this approach does not consider the semantic meaning of words or the coherence of the summary. More advanced techniques, such as using word embeddings or graph-based methods, can improve the quality of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77447a86-ce5f-4df9-9cb3-6e1a33e79e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractive_summarization(text, n_sentences=3):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Tokenize the sentences\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_freq:\n",
    "                word_freq[token] = 1\n",
    "            else:\n",
    "                word_freq[token] += 1\n",
    "\n",
    "    # Assign scores to sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        score = sum([word_freq[token] for token in sentence])\n",
    "        sentence_scores[sentences[i]] = score\n",
    "\n",
    "    # Select top sentences for the summary\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n_sentences]\n",
    "    # Combine the selected sentences to form the summary\n",
    "    summary = '. '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791a142-a079-4db3-b720-e47f5a2d6733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries = [extractive_summarization(text, n_sentences=1) for text in test_data]\n",
    "\n",
    "# Print summary for every document in the test set\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Summary {i + 1}: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044b883-3b05-4b3f-a414-ecc8e4509ec9",
   "metadata": {},
   "source": [
    "# **6. Visualization and Reporting:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed63a74-5413-4146-ab80-b6fdad05b897",
   "metadata": {},
   "source": [
    "Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d055d9d-8caf-4b2f-b490-309d27aaa9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge all the texts in the test set\n",
    "merged_test_text = ' '.join(test_data)\n",
    "_, _, lemmatized_merged_text = preprocess_text([merged_test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46818890-8183-444f-8d75-b3432b1d897e",
   "metadata": {},
   "source": [
    "## Main topic\n",
    "In this section I print the main topic identified from all the documents in the test set, so from 100 different texts, using the LDA model from gensim and my own implementation. The test set is composed by 70% space topic, so what I expect is that the model will gives me \"Space\" as main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d00bc-bec8-4128-8366-861f1dcf1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_distribution(topic_distribution, topic_mapping_func):\n",
    "    # Extract topic IDs and probabilities\n",
    "    topic_ids, probabilities = zip(*topic_distribution)\n",
    "    topic_names = [topic_mapping_func(topic_id) for topic_id in topic_ids]\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.bar(topic_names, probabilities)\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Topic Distribution in Document')\n",
    "    plt.xticks(topic_ids)\n",
    "\n",
    "    # Show the bar chart\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c6c23-4111-40f8-b55a-12b7805aac95",
   "metadata": {},
   "source": [
    "### Gensim version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c928a2-a91d-4550-9c60-55ebb242ab06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the topic distribution for the merged text\n",
    "merged_text_topic_distribution = lda_gensim.get_document_topics(dictionary.doc2bow(lemmatized_merged_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2e3c4-306d-44bb-9d91-3c915b24bf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nMain topics distribution:\")\n",
    "\n",
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(merged_text_topic_distribution):\n",
    "    sorted_topic_dist = sorted([topic_dist], key=lambda x: -x[1])\n",
    "    formatted_topics = []\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        topic_name =  topic_category_mapping(topic_id)\n",
    "        formatted_topic = f\"[{topic_id}] {topic_name} -> {probability}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "        formatted_topics_str = \" - \".join(formatted_topics)\n",
    "    print(f\"{formatted_topics_str}\")\n",
    "\n",
    "# Identify the main topic based on the highest average topic distribution\n",
    "main_topic = max(merged_text_topic_distribution, key=lambda x: x[1])[0]\n",
    "topic_name = topic_category_mapping(main_topic)\n",
    "\n",
    "# Display the main topic keywords\n",
    "main_topic_keywords = lda_gensim.show_topic(main_topic)\n",
    "print(f\"\\nMain {topic_name} keywords:\\n{main_topic_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ca5db-d346-4a85-8ad6-158c22c3ada7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_topic_distribution(merged_text_topic_distribution, topic_category_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67b287-de9a-4970-918d-01b97bc88173",
   "metadata": {},
   "source": [
    "### Scratch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbca7a6-b930-4688-be8e-5df2cc834b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_topic_distributions = lda_scratch.get_document_topics(dictionary.doc2bow(lemmatized_merged_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772c8b4-0417-4035-8dec-ae5b6b2bd370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nMain topics distribution:\")\n",
    "\n",
    "# Display the topic distribution\n",
    "sorted_topic_dist = sorted(test_topic_distributions, key=lambda x: -x[1])\n",
    "for topic_id, probability in sorted_topic_dist:\n",
    "    # Exclude probabilities under 0.2\n",
    "    #if probability < 0.2:\n",
    "    #    continue\n",
    "\n",
    "    topic_name = topic_category_mapping_scratch(topic_id)\n",
    "    formatted_topic = f\"[{topic_id}] {topic_name} -> {probability}\"\n",
    "    print(formatted_topic)\n",
    "\n",
    "# Identify the main topic based on the highest probability\n",
    "main_topic = sorted_topic_dist[0][0]\n",
    "topic_name = topic_category_mapping_scratch(main_topic)\n",
    "\n",
    "# Display the main topic keywords\n",
    "main_topic_keywords = lda_scratch.show_topic(main_topic)\n",
    "print(f\"\\nMain {topic_name} keywords:\\n{main_topic_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8cb5a-553b-4a65-b220-b96d5506655d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_topic_distribution(test_topic_distributions, topic_category_mapping_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd4c72-8148-4b4e-bf1b-3dece9cc4867",
   "metadata": {},
   "source": [
    "## Average sentiment\n",
    "In this section I print the average of all the documents in the test set, and I display the sentiments scores using a bar-chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b542c4c-9b6f-4317-86db-df392c21ed7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_text_sentiment = get_sentiment(merged_test_text)\n",
    "\n",
    "global_sentiment = merged_text_sentiment[0]\n",
    "positive_words = merged_text_sentiment[1]\n",
    "negative_words = merged_text_sentiment[2]\n",
    "\n",
    "print(f\"Global Text Sentiment: {global_sentiment}\")\n",
    "print(f\"\\nPositive words: {positive_words[:10]}\")\n",
    "print(f\"\\nNegative words: {negative_words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cf406-345f-40d0-9f83-33e1618ec4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to visualize sentiment scores\n",
    "def visualize_sentiment(sentiment_scores):\n",
    "    labels = ['Positive', 'Neutral', 'Negative']\n",
    "    values = [sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg']]\n",
    "\n",
    "    plt.bar(labels, values)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Sentiment Analysis')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_sentiment(merged_text_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3327e-30b4-43ad-a45b-68aed68b3803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sentiment_pie_chart(sentiment_scores):\n",
    "    # Extract sentiment labels and scores\n",
    "    labels, scores = zip(*sentiment_scores.items())\n",
    "\n",
    "    # Create a pie chart\n",
    "    plt.pie(scores[:-1], labels=labels[:-1], autopct='%1.1f%%', startangle=90)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title('Sentiment Distribution')\n",
    "\n",
    "    # Show the pie chart\n",
    "    plt.show()\n",
    "    \n",
    "plot_sentiment_pie_chart(merged_text_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6db6d-4211-4af8-bc32-cca1494e34ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this section I print a summary of all documents the test set, showing the relevant phrases from all the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1f6ef-e80b-41b9-8acb-7b7f956dec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = extractive_summarization(merged_test_text, n_sentences=1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907dc95-5e30-4d74-a1d4-3183af6d6213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_wordcloud([summary], \"Summary words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465cbe7-7e5f-4906-bad7-d85e43e0ab03",
   "metadata": {},
   "source": [
    "## Word distribution\n",
    "In this section I show the most frequent words in the test set. Bigger are the words showed, higher is their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905634a2-65a2-4901-8503-238999bc3fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_wordcloud(texts, title):\n",
    "    all_text = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=5, max_words=100).generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed98675-3a29-4103-960b-b83ecf221bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate word cloud\n",
    "generate_wordcloud([merged_test_text], \"All texts Word Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb9713-1938-4169-b8e1-ae7e36e9b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(positive_words, \"Positive Words Word Cloud\")\n",
    "generate_wordcloud(negative_words, \"Negative Words Word Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74377c-6310-4b3a-b2db-b68be4cbb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e6704-9bee-447a-ba5e-54bccebe7027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b273e-8131-44c9-bd52-7cbf6000f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721a5f2-71ba-4394-ba63-ce81ae1683fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd15b3-f2f2-4d6a-80e6-8d07a088efab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b5b29-b75c-45b0-b8c9-46be8762f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b7f83-085f-43c2-b664-632e0afb7915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ed18c-26f9-4993-a7f4-982cf7d6bc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
