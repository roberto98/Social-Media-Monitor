{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f544ffc-1390-4056-b2ce-d0e9da4f0cac",
   "metadata": {},
   "source": [
    "# Social Media Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c807eee-6edf-446c-8302-da94aa9e70bf",
   "metadata": {},
   "source": [
    "In this project i'll implement a social media monitor that tracks topics or trends from social media or blogs. This project can help businesses or individuals stay up-to-date with the latest developments and discussions related to their areas of interest.\n",
    "\n",
    "To implement this project, I'll follow these steps:\n",
    "\n",
    "- **1. Data Collection:** Gather data from various sources like news websites, blogs, and social media using APIs or web scraping techniques or RSS feed. In this case I'll use the 20newsgroups dataset from Sklearn that comprises around 18000 newsgroups posts on 20 topics.\n",
    "- **2. Text Preprocessing:** Clean and normalize the text data using stopword removal, stemming and lemmatization.\n",
    "- **3. Topic Modeling:** Employ topic modeling techniques like Latent Dirichlet Allocation (LDA) to identify the main topics or themes present in the collected data. This will help filter relevant content based on the topics of interest.\n",
    "- **4. Sentiment Analysis:** Determine the sentiment of the content (positive, negative, or neutral) using a rule-based approach like VADER sentiment analyzer.\n",
    "- **5. Summarization:** Generate summaries of the relevant content using extractive summarization based on word frequencies, so that users can quickly grasp the main points without reading the entire text.\n",
    "- **6. Visualization and Reporting:** Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2a2ee-b7b3-4d83-92f1-6eada6776a96",
   "metadata": {},
   "source": [
    "# **0. Import libraries:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbe65e-86d5-4379-9ddd-f036ba5453bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install pyLDAvis\n",
    "%pip install vaderSentiment\n",
    "%pip install wordcloud\n",
    "%pip install matplotlib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcc297b-6397-49e9-b2c7-4e383dfa9c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\robyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\robyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# --------------- Dataset ------------- #\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# --------------- Pre-Processing -------- #\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# --------------- LDA Model ---------- #\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "# --------------- Sentiment Analysis --------- #\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# --------------- \n",
    "\n",
    "# --------------- Visualize and Report ----------- #\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e656e-b9db-4705-84ea-edf8844b994c",
   "metadata": {},
   "source": [
    "# **1. Data Collection:**\n",
    "Now, let's load the 20newsgroup dataset and focus on the 5 categories for training:\n",
    "http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98273d9-fff1-473e-ad37-686a1a7b6579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball dataset size:  597\n",
      "Hardware dataset size:  590\n",
      "Med dataset size:  594\n",
      "Space dataset size:  593\n",
      "Guns dataset size:  546\n",
      "Crypt dataset size:  595\n"
     ]
    }
   ],
   "source": [
    "# Verify train dataset is balanced\n",
    "baseball_train = fetch_20newsgroups(subset='train', categories=['rec.sport.baseball'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Baseball dataset size: \", len(baseball_train.data))\n",
    "\n",
    "hardware_train = fetch_20newsgroups(subset='train', categories=['comp.sys.ibm.pc.hardware'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Hardware dataset size: \", len(hardware_train.data))\n",
    "\n",
    "med_train = fetch_20newsgroups(subset='train', categories=['sci.med'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Med dataset size: \", len(med_train.data))\n",
    "\n",
    "space_train = fetch_20newsgroups(subset='train', categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Space dataset size: \", len(space_train.data))\n",
    "\n",
    "guns_train = fetch_20newsgroups(subset='train', categories=['talk.politics.guns'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Guns dataset size: \", len(guns_train.data))\n",
    "\n",
    "crypt_train = fetch_20newsgroups(subset='train', categories=['sci.crypt'], remove=('headers', 'footers', 'quotes'))\n",
    "print(\"Crypt dataset size: \", len(crypt_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1313f6d9-7343-4879-81a0-9af81cb22650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'sci.med', 'sci.space', 'talk.politics.guns']\n",
      "[1 4 2 3 3 1 4 4 2 1 0 3 4 1 2 3 0 0 3 1 3 2 0 0 4 0 2 4 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset with selected categories\n",
    "train_categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'sci.med', 'sci.space', 'talk.politics.guns']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=train_categories, remove=('headers', 'footers', 'quotes'))\n",
    "#newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(newsgroups_train.target_names)\n",
    "print(newsgroups_train.target[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3904b5-5d37-4e00-9ab8-f1172e5f0426",
   "metadata": {},
   "source": [
    "For the test set, I will create a custom dataset with 70% rec.autos and 30% sci.space documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61170b2a-afdf-477f-a3f2-da74523e9b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups test dataset size:  100\n"
     ]
    }
   ],
   "source": [
    "# Create test dataset with 100 samples: 70% guns and 30% space\n",
    "n_test_docs = 100\n",
    "n_guns_docs = int(n_test_docs * 0.7)\n",
    "n_space_docs = n_test_docs - n_guns_docs\n",
    "\n",
    "# Fetch data for each category\n",
    "guns_test = fetch_20newsgroups(subset='test', categories=['talk.politics.guns'], remove=('headers', 'footers', 'quotes'))\n",
    "space_test = fetch_20newsgroups(subset='test', categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Randomly select the desired number of documents from each category\n",
    "guns_indices = np.random.choice(len(guns_test.data), n_guns_docs, replace=False)\n",
    "space_indices = np.random.choice(len(space_test.data), n_space_docs, replace=False)\n",
    "\n",
    "# Create the test dataset\n",
    "test_data = [guns_test.data[i] for i in guns_indices] + [space_test.data[i] for i in space_indices]\n",
    "test_target = np.concatenate((guns_test.target[guns_indices], space_test.target[space_indices]))\n",
    "\n",
    "# Create the newsgroups_test dataset\n",
    "newsgroups_test = {\n",
    "    'data': test_data,\n",
    "    'target': test_target,\n",
    "    'target_names': ['talk.politics.guns', 'sci.space']\n",
    "}\n",
    "\n",
    "print(\"Newsgroups test dataset size: \", len(newsgroups_test['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb09b4ca-6e40-42ea-8651-b096592ff444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the wake of the Waco denouement, I had email discussions with\n",
      "people from this group.  In particular, we discussed how cults\n",
      "operate, why the FBI might be motivated to black out news or behave\n",
      "the way it did, and what kinds of problems are involved in dealing\n",
      "with cults and similar organizations.\n",
      "\n",
      "I include an edited account of what I wrote.  The identity of my\n",
      "correspondents have (I hope) been erased.  The editing process makes\n",
      "the text choppy - sorry about that.  I've tried to retain the\n",
      "information content.\n",
      "\n",
      "Ellipses (...) indicate where text was removed.  A few of the comments\n",
      "in parentheses are new, intended to make it easier for outsiders to\n",
      "understand.\n",
      "\n",
      "These notes are preliminary - feel free to criticize.\n",
      "\n",
      "Cheers(?),\n",
      "Oded\n",
      "\n",
      "------------------------ (begin included text) -----------------------\n",
      "\n",
      "I took a course called the MADNESS OF CROWDS, ...  The course included\n",
      "cults and briefly mentioned/analyzed Jonestown.  (Did some external\n",
      "reading too).\n",
      "\n",
      "William Adorno ... edited a series of books on the psychology of\n",
      "\"evil\" mass movements...  starting with THE AUTHORITARIAN PERSONALITY,\n",
      "University of Chicago Press, 1948 ... an attempt to figure out what\n",
      "would motivate people to support fascism or be the bad guys in WWII,\n",
      "and by extension in other wars, in racial lynchings ...  I don't think\n",
      "the books are perfect, and the study of psychopathology has advanced\n",
      "..., but you can elicit Koresh types from even the first volume.  So I\n",
      "think they're onto something.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\t\tHow cult psychology works...\n",
      "\t[I'm an amateur.  Hope I'm not hopelessly naive.]\n",
      "\n",
      "So long as Koresh could \"own\" his people, he made sure they didn't\n",
      "believe there was any life out there for them, away from him.\n",
      "Otherwise, he'd (Koresh) be nothing.  During the siege, ex-Davidians\n",
      "recounted how he convinced the people in the compound they survived\n",
      "only through his intercession with God to spare their worthless souls.\n",
      "Absolutely classic brainwashing technique. ...  As long as they\n",
      "believed him, they'd ignore BATF/FBI/Child Protective Services or even\n",
      "the Red Cross asking them to come out.  After all, if they ever left\n",
      "him, God would catapult them straight to Hell, and the combined forces\n",
      "of the US gummint, with all the goodwill in the world (doubtful)\n",
      "couldn't save them for a second.  If I believed it, I'd stay and die\n",
      "too, like the folks in Jonestown.\n",
      "\n",
      "For a prosaic analogy, replace a cult leader with an estranged wife\n",
      "(or husband), and notice how many folks show up, kill the ex and then\n",
      "themselves.  That's the consequence of shattered \"cultism.\"  It really\n",
      "does happen all the time.  [By the way, the treasured ideal in such\n",
      "cases, without which life is meaningless, is the relationship, no\n",
      "matter how abusive, rather than the individual's partner.]\n",
      "\n",
      "...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "    [Why No News, Don't The Feds Owe The World An Explanation?]\n",
      "\n",
      "I agree that official explanations are in order.  I can also see good\n",
      "(?)  reasons for news blackouts.\n",
      "\n",
      "\n",
      " ... that no matter what, those people would have died, because Koresh\n",
      "made sure they believed they had no lives outside his influence.\n",
      "Hence it would make little difference when or how the FBI acted.  He\n",
      "held them hostage, as his trump against going to jail, but nothing\n",
      "would really stop him from offing them.  Even if the FBI went away!\n",
      "\n",
      "Look at history.  Rep Leo Ryan (and some staffers) visited Jonestown,\n",
      "at the request of constituents who had relatives there.  Once\n",
      "Jonestown was discovered, and even though they killed Ryan and his\n",
      "entourage ... they all killed themselves, because Jim Jones knew he'd\n",
      "be busted.  Internal arguments asking to spare the children, brought\n",
      "up by some of the women in the cult, were shouted down.  There are\n",
      "tapes...  The \"logic\" of saying that no matter how bad the gummint is, it\n",
      "wouldn't kill the kids, was shouted down as blasphemy, and the people\n",
      "who brought it up were threatened with ostracism by people who by\n",
      "THEIR OWN AVOWAL would be dead within the hour.  I suspect it's the\n",
      "same with the Branch Davidians.\n",
      "(There's a book on Jonestown by James Reston Jr., titled OUR FATHER\n",
      "WHO ART IN HELL.  I don't know whether it's good - never read it.)\n",
      "\n",
      "...\n",
      "The only way to prevent such a problem would be never to investigate\n",
      "reports of child abuse or sexual mistreatment, or organizations buying\n",
      "full-auto conversion kits or shipping hand grenades via UPS, on the\n",
      "off chance of stumbling across cults that would kill themselves. ...\n",
      "\n",
      "So, the only way the BATF/FBI could \"save\" those people, and future\n",
      "cults, is by ignoring such signs.\n",
      "\n",
      "I suppose there's another way - outsmarting Koresh and tricking him\n",
      "into letting them go, or somehow influencing \"his\" people to abandon\n",
      "him while he owns most of their means of communication with the world.\n",
      "... a mighty tough row to hoe. ...  I suspect the FBI tried to do that\n",
      "with blackouts, noise and other sensory insults.  However, maybe\n",
      "they're not very sophisticated, or maybe the job is impossible.  It's\n",
      "certainly possible the guy running the show was a jerk.\n",
      "\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "\t[Why the FBI might want to blackout during and coverup after]\n",
      "\n",
      "    ... - if they were doing a poor job of weaning the BD's from\n",
      "Koresh, they'd want to keep it quiet so they wouldn't be embarrassed.\n",
      "\n",
      "    ... - if they were trying to wean the BD's from Koresh, they'd\n",
      "want to keep it quiet so he couldn't outflank them, or well-meaning\n",
      "boneheads from ANY point of view wouldn't screw it up. ...  I _hate_\n",
      "playing chess when the world screams in my face, especially if at\n",
      "checkmate time people really die, and I could be blamed.\n",
      "\n",
      "...\n",
      "\n",
      "I don't think ignoring such incidents is a workable policy, unless you\n",
      "deny the FBI, BATF, Child Protective Services (of whatever stripe) and\n",
      "the rest of the gummint should exist at all. \n",
      "\n",
      "\t\t(the end)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80e6e70-ea46-437a-933b-4c36c5603d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  2920\n",
      "Train topics are:\n",
      " ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'sci.med', 'sci.space', 'talk.politics.guns']\n",
      "\n",
      "Test dataset size:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size: \", len(newsgroups_train.data))\n",
    "print(\"Train topics are:\\n\",newsgroups_train.target_names)\n",
    "\n",
    "print(\"\\nTest dataset size: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51b685-8c16-42eb-b823-7824e7055ad5",
   "metadata": {},
   "source": [
    "# **2. Text Preprocessing:** \n",
    "Clean and normalize the text data using tokenization, stopword removal, and stemming/lemmatization. I'll use the `nltk` library for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55868e82-bb4d-4517-929d-b4303ef2372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    # Remove punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def tokenize(text):\n",
    "        return [word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    def lemmatize(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_data = [tokenize(text) for text in data]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_data = [[stemmer.stem(token) for token in text] for text in tokenized_data]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatized_data = [lemmatize(text) for text in tokenized_data]\n",
    "\n",
    "    return stemmed_data, lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74d0439-7fdd-4fe0-94a5-5c5187f04e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmed_train_data, lemmatized_train_data = preprocess_text(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2bf5d2-7ee1-46f8-997f-3b0c958f0549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brook', 'robinson', 'defens', 'liabil', 'ted', 'william', 'weak', 'hitter', 'even', 'great', 'player', 'declin', 'age']]\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae15115-95df-4a7b-9df3-57bae75167db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brook', 'robinson', 'defensive', 'liability', 'ted', 'williams', 'weak', 'hitter', 'even', 'great', 'player', 'decline', 'age']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6db78ca-0f71-4ab0-a31c-7b67dbade66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wake', 'waco', 'denouement', 'email', 'discussion', 'people', 'group', 'particular', 'discussed', 'cult', 'operate', 'fbi', 'might', 'motivated', 'black', 'news', 'behave', 'way', 'kind', 'problem', 'involved', 'dealing', 'cult', 'similar', 'organization', 'include', 'edited', 'account', 'wrote', 'identity', 'correspondent', 'hope', 'erased', 'editing', 'process', 'make', 'text', 'choppy', 'sorry', 'tried', 'retain', 'information', 'content', 'ellipsis', 'indicate', 'text', 'removed', 'comment', 'parenthesis', 'new', 'intended', 'make', 'easier', 'outsider', 'understand', 'note', 'preliminary', 'feel', 'free', 'criticize', 'cheer', 'oded', 'begin', 'included', 'text', 'took', 'course', 'called', 'madness', 'crowd', 'course', 'included', 'cult', 'briefly', 'jonestown', 'external', 'reading', 'william', 'adorno', 'edited', 'series', 'book', 'psychology', 'evil', 'mass', 'movement', 'starting', 'authoritarian', 'personality', 'university', 'chicago', 'press', '1948', 'attempt', 'figure', 'would', 'motivate', 'people', 'support', 'fascism', 'bad', 'guy', 'wwii', 'extension', 'war', 'racial', 'lynching', 'think', 'book', 'perfect', 'study', 'psychopathology', 'advanced', 'elicit', 'koresh', 'type', 'even', 'first', 'volume', 'think', 'onto', 'something', 'cult', 'psychology', 'work', 'amateur', 'hope', 'hopelessly', 'naive', 'long', 'koresh', 'could', 'people', 'made', 'sure', 'believe', 'life', 'away', 'otherwise', 'koresh', 'nothing', 'siege', 'recounted', 'convinced', 'people', 'compound', 'survived', 'intercession', 'god', 'spare', 'worthless', 'soul', 'absolutely', 'classic', 'brainwashing', 'technique', 'long', 'believed', 'ignore', 'protective', 'service', 'even', 'red', 'cross', 'asking', 'come', 'ever', 'left', 'god', 'would', 'catapult', 'straight', 'hell', 'combined', 'force', 'u', 'gummint', 'goodwill', 'world', 'doubtful', 'could', 'save', 'second', 'believed', 'stay', 'die', 'like', 'folk', 'jonestown', 'prosaic', 'analogy', 'replace', 'cult', 'leader', 'estranged', 'wife', 'husband', 'notice', 'many', 'folk', 'show', 'kill', 'ex', 'consequence', 'shattered', 'cultism', 'really', 'happen', 'time', 'way', 'treasured', 'ideal', 'case', 'without', 'life', 'meaningless', 'relationship', 'matter', 'abusive', 'rather', 'individual', 'partner', 'news', 'fed', 'owe', 'world', 'explanation', 'agree', 'official', 'explanation', 'order', 'also', 'see', 'good', 'reason', 'news', 'blackout', 'matter', 'people', 'would', 'died', 'koresh', 'made', 'sure', 'believed', 'life', 'outside', 'influence', 'hence', 'would', 'make', 'little', 'difference', 'fbi', 'acted', 'held', 'hostage', 'trump', 'going', 'jail', 'nothing', 'would', 'really', 'stop', 'offing', 'even', 'fbi', 'went', 'away', 'look', 'history', 'rep', 'leo', 'ryan', 'staffer', 'visited', 'jonestown', 'request', 'constituent', 'relative', 'jonestown', 'discovered', 'even', 'though', 'killed', 'ryan', 'entourage', 'killed', 'jim', 'jones', 'knew', 'busted', 'internal', 'argument', 'asking', 'spare', 'child', 'brought', 'woman', 'cult', 'shouted', 'tape', 'logic', 'saying', 'matter', 'bad', 'gummint', 'would', 'kill', 'kid', 'shouted', 'blasphemy', 'people', 'brought', 'threatened', 'ostracism', 'people', 'avowal', 'would', 'dead', 'within', 'hour', 'suspect', 'branch', 'davidians', 'book', 'jonestown', 'james', 'reston', 'titled', 'father', 'art', 'hell', 'know', 'whether', 'good', 'never', 'read', 'way', 'prevent', 'problem', 'would', 'never', 'investigate', 'report', 'child', 'abuse', 'sexual', 'mistreatment', 'organization', 'buying', 'conversion', 'kit', 'shipping', 'hand', 'grenade', 'via', 'ups', 'chance', 'stumbling', 'across', 'cult', 'would', 'kill', 'way', 'could', 'save', 'people', 'future', 'cult', 'ignoring', 'sign', 'suppose', 'another', 'way', 'outsmarting', 'koresh', 'tricking', 'letting', 'go', 'somehow', 'influencing', 'people', 'abandon', 'owns', 'mean', 'communication', 'world', 'mighty', 'tough', 'row', 'hoe', 'suspect', 'fbi', 'tried', 'blackout', 'noise', 'sensory', 'insult', 'however', 'maybe', 'sophisticated', 'maybe', 'job', 'impossible', 'certainly', 'possible', 'guy', 'running', 'show', 'jerk', 'fbi', 'might', 'want', 'blackout', 'coverup', 'poor', 'job', 'weaning', 'bd', 'koresh', 'want', 'keep', 'quiet', 'would', 'embarrassed', 'trying', 'wean', 'bd', 'koresh', 'want', 'keep', 'quiet', 'could', 'outflank', 'bonehead', 'point', 'view', 'would', 'screw', 'playing', 'chess', 'world', 'scream', 'face', 'especially', 'checkmate', 'time', 'people', 'really', 'die', 'could', 'blamed', 'think', 'ignoring', 'incident', 'workable', 'policy', 'unless', 'deny', 'fbi', 'batf', 'child', 'protective', 'service', 'whatever', 'stripe', 'rest', 'gummint', 'exist', 'end']]\n"
     ]
    }
   ],
   "source": [
    "stemmed_test_data, lemmatized_test_data = preprocess_text(test_data)\n",
    "print(lemmatized_test_data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19967faa-f869-497b-beea-bdf329f79f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **3. Topic Modeling:** \n",
    "Apply Latent Dirichlet Allocation (LDA) to identify the main topics in the collected data. In this part I'll use a scratch implementation and compare it with `gensim` library version. To evaluate both models on the testing set I compute the coherence scores.\n",
    "\n",
    "Remember, for topic modeling, you can train your model on any similar corpus of text documents. It doesn't necessarily have to contain the same topics as your unseen documents but having some overlap would likely improve performance. For example, if you're looking to categorize social media posts from a specific platform or about a specific subject, you would ideally use a training set gathered from the same or similar platform/subject.\n",
    "\n",
    "However, if you want to train an LDA model on the specific topics you mentioned, you would need a dataset that contains a substantial number of documents related to these topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af215bc7-7375-4f5e-8f35-bb80c39ad4ec",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf18aa-5e2f-445f-89a9-bc8d498bbfbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Latent Dirichlet Allocation (LDA) is a generative probabilistic model used in topic modeling. It is a statistical model that allows us to discover latent topics within a collection of documents. LDA assumes that each document in the collection is a mixture of various topics, and each topic is a distribution over words.\n",
    "\n",
    "It's an unsupervised learning method, meaning that it generates a probabilistic model to identify groups of topics without the need for known class labels. It uses only the distribution of words to mathematically model topic.\n",
    "\n",
    "Here's a step-by-step explanation of how LDA works:\n",
    "\n",
    "1. **Initialization**: Choose the number of topics K to extract from the document collection and randomly assign each word in each document to one of the K topics.\n",
    "\n",
    "2. **Iteration**: Iterate through each word in each document and reassign the word to a topic based on: the proportion of words in the document that belong to the topic, and the proportion of occurrences of the word across all documents that belong to the topic.\n",
    "   - For each document d:\n",
    "     - For each word w in document d:\n",
    "       - Calculate two probabilities:\n",
    "         - P(topic t | document d): Proportion of words in document d that are currently assigned to topic t.\n",
    "         - P(word w | topic t): Proportion of assignments to topic t over all documents that come from word w.\n",
    "       - Reassign word w to a new topic based on the probabilities calculated above.\n",
    "   \n",
    "   - Repeat the above step for a fixed number of iterations or until convergence.\n",
    "\n",
    "3. **Output**: Repeat step 2 for a certain number of iterations or until convergence. LDA provides two main outputs:\n",
    "   - The distribution of topics in each document.\n",
    "   - The distribution of words in each topic.\n",
    "\n",
    "These distributions can be used to interpret the topics and analyze the relationships between documents and topics.\n",
    "\n",
    "\n",
    "\n",
    "LDA assumes that documents are generated in the following way:\n",
    "- Choose the number of words in the document from a Poisson distribution.\n",
    "- Choose a topic mixture for the document from a Dirichlet distribution.\n",
    "- For each word in the document:\n",
    "  - Choose a topic from the topic mixture.\n",
    "  - Choose a word from the topic's word distribution.\n",
    "\n",
    "LDA is widely used in natural language processing and text mining tasks, such as document clustering, document classification, and information retrieval. It helps uncover the underlying themes or topics in a collection of documents, making it easier to analyze and organize large amounts of textual data[1].\n",
    "\n",
    "Please note that the search results provided additional papers and applications related to LDA, which you can explore for more specific information and use cases.\n",
    "\n",
    "Citations:\n",
    "[1] https://www.semanticscholar.org/paper/b98a4076b48552691bb99290106a378e483cdfca\n",
    "[2] https://www.semanticscholar.org/paper/03ba268430128916e195e8d1a88c761f3c9d7578\n",
    "[3] https://arxiv.org/abs/1309.3421\n",
    "[4] https://www.semanticscholar.org/paper/c80db2cd1b127ec86060ad018c04cd0c48075ae3\n",
    "[5] https://www.semanticscholar.org/paper/1713b2a9291d76c02feb49376422d800d5e44888\n",
    "[6] https://www.semanticscholar.org/paper/59c902e7797889bad1f731205a409ade2913199a\n",
    "\n",
    "\n",
    "The documents can come from any domain as long as they contain text. For example, they could be customer reviews, news articles, research papers, social media posts, etc. The words in the documents are collected into n-grams (a contiguous sequence of n items from a given sample of text or speech) and used to create a dictionary. This dictionary is then used to train the LDA model. \n",
    "\n",
    "It's important to note that the text in the documents should be preprocessed before being used for training the LDA model. This preprocessing can include removing stop words (commonly used words such as 'the', 'a', 'an', 'in'), lowercasing all the words, and lemmatizing the words (reducing inflectional forms and sometimes derivationally related forms of a word to a common base form)\n",
    "\n",
    "When configuring the LDA model, some parameters that can be set include the rho parameter (a prior probability for the sparsity of topic distributions), the alpha parameter (a prior probability for the sparsity of per-document topic weights), the estimated number of documents, the size of the batch, the initial value of iteration used in learning update schedule, the power applied to the iteration during updates, and the number of passes over the data\n",
    "\n",
    "\n",
    "As a result of the training, each document will be represented as a combination of topics, and each topic will be represented as a distribution over words. This can be used to classify new documents, identify related terms, and create recommendations.\n",
    "\n",
    "![LDA](https://www.researchgate.net/profile/Diego-Buenano-Fernandez/publication/339368709/figure/fig1/AS:860489982689280@1582168207260/Schematic-of-LDA-algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55014f19-2030-4e8f-a44d-1eabff08bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of topics to extract from documents\n",
    "num_topics = 5\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(lemmatized_train_data)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "train_corpus = [dictionary.doc2bow(text) for text in lemmatized_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5461a926-5734-49ed-bdd6-61d3e78b377f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('age', 1),\n",
       "  ('brook', 1),\n",
       "  ('decline', 1),\n",
       "  ('defensive', 1),\n",
       "  ('even', 1),\n",
       "  ('great', 1),\n",
       "  ('hitter', 1),\n",
       "  ('liability', 1),\n",
       "  ('player', 1),\n",
       "  ('robinson', 1),\n",
       "  ('ted', 1),\n",
       "  ('weak', 1),\n",
       "  ('williams', 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human-readable format of corpus (term-frequency)\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in train_corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29786fa-50a0-4530-9eea-7749b303490a",
   "metadata": {},
   "source": [
    "## Gensim version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2490f1e-7e06-4524-913a-a619e0ad125c",
   "metadata": {},
   "source": [
    "**Building the Topic Model**\n",
    "\n",
    "Let's now build the topic model. We'll define 5 topics to start with. The hyperparameter alpha affects sparsity of the document-topic (theta) distributions, whose default value is 1. Similarly, the hyperparameter eta can also be specified, which affects the topic-word distribution's sparsity.\n",
    "\n",
    "https://www.kaggle.com/code/datajameson/topic-modelling-nlp-amazon-reviews-bbc-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c17fee-75ef-4066-96aa-6429ed741ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the LDA model using the processed training data\n",
    "lda_model = LdaModel(corpus=train_corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1b505b-303c-43d0-bb3e-741b1d90e580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping between topic number and category name\n",
    "topic_category_mapping = {\n",
    "    0: 'Guns',\n",
    "    1: 'Baseball',\n",
    "    2: 'Med',\n",
    "    3: 'Hardware',\n",
    "    4: 'Space'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0cab14b-21bb-4c13-b7b1-2132cbede10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 (Guns) | Words: ['would', 'one', 'gun', 'like', 'get', 'know', 'time', 'people', 'problem', 'thing']\n",
      "\n",
      "Topic 1 (Baseball) | Words: ['would', 'year', 'one', 'game', 'think', 'get', 'good', 'last', 'team', 'well']\n",
      "\n",
      "Topic 2 (Med) | Words: ['center', 'research', 'medical', 'space', 'health', '1993', 'disease', 'cancer', 'information', 'use']\n",
      "\n",
      "Topic 3 (Hardware) | Words: ['drive', '1', '0', '2', 'card', 'controller', '3', 'scsi', 'disk', '4']\n",
      "\n",
      "Topic 4 (Space) | Words: ['space', 'launch', 'satellite', 'system', 'data', 'nasa', 'also', 'orbit', 'program', 'year']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print topics and associated category names\n",
    "for topic_num, topic in lda_model.show_topics(num_topics=num_topics, formatted=False):\n",
    "    topic_words = [word for word, _ in topic]\n",
    "    category_name = topic_category_mapping.get(topic_num, f'Unknown Category {topic_num}')\n",
    "    print(f\"Topic {topic_num} ({category_name}) | Words: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b118af9-55a4-4c8b-b9f6-ee30f00c1031",
   "metadata": {},
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, train_corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c463c-c77d-4c08-b5ee-82e5be21c0b6",
   "metadata": {},
   "source": [
    "Let's now evaluate the model using coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed01c4d-cd3b-4b94-9962-76e23da282e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the coherence score to evaluate the model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized_train_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c504d68-c3d6-486a-8a76-6131b19f2a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_distribution(lda_model, dictionary, document):\n",
    "    # Preprocess the document\n",
    "    _, preprocessed_document = preprocess_text([document])\n",
    "    # Convert the document into BoW format\n",
    "    bow_document = dictionary.doc2bow(preprocessed_document[0]) # Here we are assuming that preprocessed_document is a list of lists\n",
    "    \n",
    "    # Get the topic distribution\n",
    "    topic_distribution = lda_model.get_document_topics(bow_document, minimum_probability=0.2)\n",
    "    return topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c3d16a-edd6-4548-9b6c-032c9c2c6960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 topics : [0] Guns 0.50 - [1] Baseball 0.43\n",
      "Document 2 topics : [0] Guns 0.90\n",
      "Document 3 topics : [1] Baseball 0.55 - [0] Guns 0.45\n",
      "Document 4 topics : [0] Guns 0.94\n",
      "Document 5 topics : [1] Baseball 0.48 - [0] Guns 0.34\n",
      "Document 6 topics : [0] Guns 0.67 - [1] Baseball 0.30\n",
      "Document 7 topics : [0] Guns 0.60 - [1] Baseball 0.40\n",
      "Document 8 topics : [0] Guns 0.76\n",
      "Document 9 topics : [0] Guns 0.85\n",
      "Document 10 topics : [0] Guns 0.76\n",
      "Document 11 topics : [0] Guns 0.72\n",
      "Document 12 topics : [0] Guns 0.90\n",
      "Document 13 topics : [0] Guns 0.73\n",
      "Document 14 topics : [1] Baseball 0.65 - [0] Guns 0.35\n",
      "Document 15 topics : [1] Baseball 0.49 - [0] Guns 0.48\n",
      "Document 16 topics : [0] Guns 0.91\n",
      "Document 17 topics : [4] Space 0.67 - [1] Baseball 0.31\n",
      "Document 18 topics : [0] Guns 0.50 - [1] Baseball 0.45\n",
      "Document 19 topics : [0] Guns 0.74 - [1] Baseball 0.26\n",
      "Document 20 topics : [0] Guns 0.97\n",
      "Document 21 topics : [0] Guns 0.61 - [1] Baseball 0.26\n",
      "Document 22 topics : [0] Guns 0.94\n",
      "Document 23 topics : [0] Guns 0.73\n",
      "Document 24 topics : [1] Baseball 0.70\n",
      "Document 25 topics : [0] Guns 0.55 - [1] Baseball 0.42\n",
      "Document 26 topics : [0] Guns 0.69 - [1] Baseball 0.30\n",
      "Document 27 topics : [0] Guns 0.61 - [1] Baseball 0.26\n",
      "Document 28 topics : [0] Guns 0.82\n",
      "Document 29 topics : [0] Guns 0.77\n",
      "Document 30 topics : [0] Guns 0.50 - [1] Baseball 0.47\n",
      "Document 31 topics : [0] Guns 0.96\n",
      "Document 32 topics : [0] Guns 0.82\n",
      "Document 33 topics : [0] Guns 0.63 - [1] Baseball 0.37\n",
      "Document 34 topics : [0] Guns 0.73 - [1] Baseball 0.26\n",
      "Document 35 topics : [0] Guns 0.65 - [1] Baseball 0.35\n",
      "Document 36 topics : [1] Baseball 0.88\n",
      "Document 37 topics : [0] Guns 0.94\n",
      "Document 38 topics : [0] Guns 0.57 - [1] Baseball 0.41\n",
      "Document 39 topics : [0] Guns 0.45 - [1] Baseball 0.25\n",
      "Document 40 topics : [0] Guns 0.45 - [1] Baseball 0.25\n",
      "Document 41 topics : [0] Guns 0.84\n",
      "Document 42 topics : [0] Guns 0.90\n",
      "Document 43 topics : [1] Baseball 0.72 - [0] Guns 0.28\n",
      "Document 44 topics : [1] Baseball 0.54 - [0] Guns 0.45\n",
      "Document 45 topics : [1] Baseball 0.81\n",
      "Document 46 topics : [0] Guns 0.52 - [1] Baseball 0.48\n",
      "Document 47 topics : [0] Guns 0.53 - [1] Baseball 0.27\n",
      "Document 48 topics : [0] Guns 0.90\n",
      "Document 49 topics : [1] Baseball 0.95\n",
      "Document 50 topics : [0] Guns 0.45 - [1] Baseball 0.25\n",
      "Document 51 topics : [0] Guns 0.92\n",
      "Document 52 topics : [0] Guns 0.53 - [4] Space 0.24 - [1] Baseball 0.22\n",
      "Document 53 topics : [1] Baseball 0.47 - [0] Guns 0.47\n",
      "Document 54 topics : [0] Guns 0.62 - [4] Space 0.32\n",
      "Document 55 topics : [0] Guns 0.64 - [1] Baseball 0.23\n",
      "Document 56 topics : [0] Guns 0.81\n",
      "Document 57 topics : [0] Guns 0.54 - [1] Baseball 0.42\n",
      "Document 58 topics : [1] Baseball 0.98\n",
      "Document 59 topics : [0] Guns 0.75\n",
      "Document 60 topics : [0] Guns 0.81\n",
      "Document 61 topics : [0] Guns 0.88\n",
      "Document 62 topics : [0] Guns 0.71 - [1] Baseball 0.29\n",
      "Document 63 topics : [1] Baseball 0.74 - [0] Guns 0.26\n",
      "Document 64 topics : [0] Guns 0.46 - [1] Baseball 0.45\n",
      "Document 65 topics : [1] Baseball 0.60 - [0] Guns 0.40\n",
      "Document 66 topics : [0] Guns 0.94\n",
      "Document 67 topics : [0] Guns 0.62 - [1] Baseball 0.33\n",
      "Document 68 topics : [0] Guns 0.86\n",
      "Document 69 topics : [0] Guns 0.57 - [1] Baseball 0.42\n",
      "Document 70 topics : [0] Guns 0.74 - [1] Baseball 0.22\n",
      "Document 71 topics : [1] Baseball 0.37 - [0] Guns 0.35 - [4] Space 0.28\n",
      "Document 72 topics : [0] Guns 0.45 - [1] Baseball 0.25\n",
      "Document 73 topics : [0] Guns 0.58 - [4] Space 0.38\n",
      "Document 74 topics : [0] Guns 0.76\n",
      "Document 75 topics : [0] Guns 0.52 - [4] Space 0.42\n",
      "Document 76 topics : [0] Guns 0.43 - [4] Space 0.23 - [1] Baseball 0.21\n",
      "Document 77 topics : [0] Guns 0.54 - [4] Space 0.40\n",
      "Document 78 topics : [0] Guns 0.53 - [4] Space 0.45\n",
      "Document 79 topics : [4] Space 0.49 - [0] Guns 0.26\n",
      "Document 80 topics : [0] Guns 0.51 - [4] Space 0.27\n",
      "Document 81 topics : [1] Baseball 0.49 - [0] Guns 0.37\n",
      "Document 82 topics : [0] Guns 0.78 - [1] Baseball 0.22\n",
      "Document 83 topics : [1] Baseball 0.48 - [0] Guns 0.35\n",
      "Document 84 topics : [4] Space 0.72 - [1] Baseball 0.26\n",
      "Document 85 topics : [0] Guns 0.62 - [4] Space 0.32\n",
      "Document 86 topics : [4] Space 0.49 - [0] Guns 0.48\n",
      "Document 87 topics : [0] Guns 0.55 - [4] Space 0.44\n",
      "Document 88 topics : [0] Guns 0.62 - [1] Baseball 0.29\n",
      "Document 89 topics : [0] Guns 0.55 - [4] Space 0.37\n",
      "Document 90 topics : [1] Baseball 0.47 - [0] Guns 0.42\n",
      "Document 91 topics : [0] Guns 0.38 - [1] Baseball 0.35 - [4] Space 0.27\n",
      "Document 92 topics : [0] Guns 0.65 - [4] Space 0.23\n",
      "Document 93 topics : [1] Baseball 0.91\n",
      "Document 94 topics : [4] Space 0.42 - [2] Med 0.33\n",
      "Document 95 topics : [0] Guns 0.38 - [4] Space 0.35 - [1] Baseball 0.25\n",
      "Document 96 topics : [0] Guns 0.70\n",
      "Document 97 topics : [0] Guns 0.45 - [1] Baseball 0.25\n",
      "Document 98 topics : [0] Guns 0.72 - [4] Space 0.27\n",
      "Document 99 topics : [0] Guns 0.62 - [2] Med 0.20\n",
      "Document 100 topics : [4] Space 0.60 - [0] Guns 0.39\n"
     ]
    }
   ],
   "source": [
    "test_topic_distributions = [get_topic_distribution(lda_model, dictionary, text) for text in test_data]\n",
    "\n",
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(test_topic_distributions):\n",
    "    # Sort the topic distribution by probability in descending order\n",
    "    sorted_topic_dist = sorted(topic_dist, key=lambda x: -x[1])\n",
    "    # Create a list to store the formatted topics\n",
    "    formatted_topics = []\n",
    "\n",
    "    # Format and store each topic\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        # Associate label to the topic\n",
    "        category_name = topic_category_mapping.get(topic_id, f'Unknown Category {topic_id}')\n",
    "\n",
    "        formatted_topic = f\"[{topic_id}] {category_name} {probability:.2f}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "\n",
    "    # Join the formatted topics into a string\n",
    "    formatted_topics_str = \" - \".join(formatted_topics)\n",
    "\n",
    "    print(f\"Document {i + 1} topics : {formatted_topics_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af18d0f-8342-4493-bd80-c96a57a9ca6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scratch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e045ab-3e4c-4932-8246-df8969ba9afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the input corpus is in the Gensim bag-of-words format, which is a list of tuples (word index, word count)\n",
    "def lda_from_scratch(corpus, num_topics, num_iterations=100, alpha=0.1, beta=0.1):\n",
    "    # Initialize topic assignments randomly\n",
    "    topic_assignments = [[random.randint(0, num_topics - 1) for _ in doc] for doc in corpus]\n",
    "\n",
    "    # Initialize topic-word and document-topic count matrices\n",
    "    num_words = max([word_idx for doc in corpus for word_idx, _ in doc]) + 1\n",
    "    topic_word_counts = np.zeros((num_topics, num_words))\n",
    "    doc_topic_counts = np.zeros((len(corpus), num_topics))\n",
    "\n",
    "    # Count initial topic assignments\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for word_idx, (word, count) in enumerate(doc):\n",
    "            topic = topic_assignments[doc_idx][word_idx]\n",
    "            topic_word_counts[topic][word] += count\n",
    "            doc_topic_counts[doc_idx][topic] += count\n",
    "\n",
    "    # Perform Gibbs sampling\n",
    "    for it in range(num_iterations):\n",
    "        doc_topic_sums = doc_topic_counts.sum(axis=1)\n",
    "        topic_word_sums = topic_word_counts.sum(axis=1)\n",
    "        \n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for word_idx, (word, count) in enumerate(doc):\n",
    "                # Remove current topic assignment\n",
    "                old_topic = topic_assignments[doc_idx][word_idx]\n",
    "                # Decrement counts for old topic assignment\n",
    "                topic_word_counts[old_topic][word] -= count\n",
    "                doc_topic_counts[doc_idx][old_topic] -= count\n",
    "                doc_topic_sums[doc_idx] -= count\n",
    "                topic_word_sums[old_topic] -= count\n",
    "\n",
    "                # Compute probabilities for each topic (conditional distribution for the word)\n",
    "                p_topic_given_doc = (doc_topic_counts[doc_idx, :] + alpha) / (doc_topic_sums[doc_idx] + num_topics * alpha)\n",
    "                p_word_given_topic = (topic_word_counts[:, word] + beta) / (topic_word_sums + num_words * beta)\n",
    "                probabilities = p_topic_given_doc * p_word_given_topic\n",
    "\n",
    "                # Normalize probabilities\n",
    "                probabilities /= probabilities.sum()\n",
    "\n",
    "                # Sample a new topic assignment\n",
    "                new_topic = np.random.choice(num_topics, p=probabilities)\n",
    "                topic_assignments[doc_idx][word_idx] = new_topic\n",
    "\n",
    "                # Update counts for new topic assignment\n",
    "                topic_word_counts[new_topic][word] += count\n",
    "                doc_topic_counts[doc_idx][new_topic] += count\n",
    "                doc_topic_sums[doc_idx] += count\n",
    "                topic_word_sums[new_topic] += count\n",
    "\n",
    "        print(f\"Iteration {it}\")\n",
    "\n",
    "    return topic_word_counts, doc_topic_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59755ced-43af-4030-b419-93a560e733ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30\n",
    "topic_word_counts, doc_topic_counts = lda_from_scratch(train_corpus, num_topics, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f4d3b89-6d32-48ff-8663-feb8d9c3ab06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_topic_distribution_scratch(document, dictionary, topic_word_counts, doc_topic_counts, alpha=0.1, beta=0.1):\n",
    "    # Preprocess the document\n",
    "    _, preprocessed_document = preprocess_text([document])\n",
    "    # Convert the document into a bag-of-words representation\n",
    "    bow_document = dictionary.doc2bow(preprocessed_document[0])  # Assuming preprocessed_document is a list of lists\n",
    "    \n",
    "    num_topics, num_words = topic_word_counts.shape\n",
    "    doc_topic_sums = doc_topic_counts.sum(axis=1)\n",
    "    topic_word_sums = topic_word_counts.sum(axis=1)\n",
    "\n",
    "    doc_topic_counts_inferred = np.zeros(num_topics)\n",
    "\n",
    "    for word, count in bow_document:\n",
    "        p_topic_given_doc = (doc_topic_counts_inferred + alpha) / (doc_topic_sums.sum() + num_topics * alpha)\n",
    "        p_word_given_topic = (topic_word_counts[:, word] + beta) / (topic_word_sums + num_words * beta)\n",
    "        probabilities = p_topic_given_doc * p_word_given_topic\n",
    "\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        for topic in range(num_topics):\n",
    "            doc_topic_counts_inferred[topic] += count * probabilities[topic]\n",
    "\n",
    "    return doc_topic_counts_inferred\n",
    "\n",
    "\n",
    "def infer_topic_distribution_scratch(document, dictionary, topic_word_counts, doc_topic_counts, alpha=0.1, beta=0.1):\n",
    "    # Preprocess the document\n",
    "    _, preprocessed_document = preprocess_text([document])\n",
    "    # Convert the document into a bag-of-words representation\n",
    "    bow_document = dictionary.doc2bow(preprocessed_document[0])  # Assuming preprocessed_document is a list of lists\n",
    "    \n",
    "    num_topics, num_words = topic_word_counts.shape\n",
    "    topic_word_sums = topic_word_counts.sum(axis=1)\n",
    "\n",
    "    doc_topic_counts_inferred = np.zeros(num_topics)\n",
    "\n",
    "    for word, count in bow_document:\n",
    "        p_topic_given_doc = (doc_topic_counts_inferred + alpha) / (doc_topic_counts_inferred.sum() + num_topics * alpha)\n",
    "        p_word_given_topic = (topic_word_counts[:, word] + beta) / (topic_word_sums + num_words * beta)\n",
    "        probabilities = p_topic_given_doc * p_word_given_topic\n",
    "\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        for topic in range(num_topics):\n",
    "            doc_topic_counts_inferred[topic] += count * probabilities[topic]\n",
    "\n",
    "    # Normalize the inferred topic counts\n",
    "    #doc_topic_counts_inferred /= doc_topic_counts_inferred.sum()\n",
    "\n",
    "    return doc_topic_counts_inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "591bb77d-6310-4060-9da4-286c3b201c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_topic_distributions = [infer_topic_distribution_scratch(text, dictionary, topic_word_counts, doc_topic_counts) for text in test_data]\n",
    "#print(test_topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "180cd9f9-b81e-4e88-ae04-397f2c697603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 topics : [0] Guns 294.47 - [3] Hardware 105.62 - [1] Baseball 21.52 - [4] Space 15.14 - [2] Med 2.25\n",
      "Document 2 topics : [3] Hardware 6.74 - [0] Guns 5.78 - [1] Baseball 3.38 - [4] Space 1.88 - [2] Med 0.22\n",
      "Document 3 topics : [0] Guns 64.80 - [3] Hardware 14.44 - [1] Baseball 4.34 - [4] Space 1.27 - [2] Med 0.15\n",
      "Document 4 topics : [0] Guns 14.29 - [3] Hardware 12.34 - [1] Baseball 5.06 - [2] Med 0.72 - [4] Space 0.59\n",
      "Document 5 topics : [0] Guns 16.91 - [3] Hardware 3.11 - [2] Med 2.14 - [4] Space 1.99 - [1] Baseball 0.85\n",
      "Document 6 topics : [0] Guns 78.92 - [3] Hardware 61.61 - [4] Space 18.66 - [1] Baseball 5.81 - [2] Med 3.00\n",
      "Document 7 topics : [0] Guns 16.52 - [3] Hardware 14.75 - [1] Baseball 0.44 - [4] Space 0.24\n",
      "Document 8 topics : [0] Guns 22.27 - [3] Hardware 7.66 - [1] Baseball 2.43 - [4] Space 0.53 - [2] Med 0.11\n",
      "Document 9 topics : [0] Guns 30.70 - [3] Hardware 17.43 - [4] Space 4.65 - [1] Baseball 2.83 - [2] Med 0.40\n",
      "Document 10 topics : [0] Guns 39.23 - [3] Hardware 24.04 - [1] Baseball 10.92 - [4] Space 6.25 - [2] Med 1.56\n",
      "Document 11 topics : [0] Guns 476.41 - [3] Hardware 408.36 - [2] Med 53.45 - [1] Baseball 21.19 - [4] Space 19.59\n",
      "Document 12 topics : [0] Guns 11.86 - [3] Hardware 8.57 - [4] Space 2.61 - [1] Baseball 0.95\n",
      "Document 13 topics : [3] Hardware 21.43 - [0] Guns 16.36 - [4] Space 8.62 - [1] Baseball 7.19 - [2] Med 6.39\n",
      "Document 14 topics : [0] Guns 62.91 - [3] Hardware 25.03 - [1] Baseball 5.94 - [4] Space 0.87 - [2] Med 0.25\n",
      "Document 15 topics : [3] Hardware 30.56 - [0] Guns 24.34 - [4] Space 1.55 - [2] Med 1.54 - [1] Baseball 1.01\n",
      "Document 16 topics : [0] Guns 39.79 - [3] Hardware 23.08 - [4] Space 2.26 - [1] Baseball 1.68 - [2] Med 1.18\n",
      "Document 17 topics : [0] Guns 4.95 - [4] Space 2.73 - [2] Med 2.65 - [1] Baseball 2.29 - [3] Hardware 1.38\n",
      "Document 18 topics : [0] Guns 95.89 - [3] Hardware 61.12 - [2] Med 18.68 - [1] Baseball 3.89 - [4] Space 2.42\n",
      "Document 19 topics : [0] Guns 87.74 - [3] Hardware 8.49 - [1] Baseball 7.67 - [4] Space 2.72 - [2] Med 0.38\n",
      "Document 20 topics : [0] Guns 3.73 - [3] Hardware 3.65 - [1] Baseball 1.18 - [2] Med 0.31 - [4] Space 0.13\n",
      "Document 21 topics : [0] Guns 231.23 - [3] Hardware 133.49 - [4] Space 41.94 - [2] Med 13.44 - [1] Baseball 12.90\n",
      "Document 22 topics : [0] Guns 2.57 - [3] Hardware 0.86 - [1] Baseball 0.43 - [4] Space 0.12\n",
      "Document 23 topics : [3] Hardware 120.48 - [0] Guns 100.11 - [4] Space 6.35 - [1] Baseball 4.01 - [2] Med 2.05\n",
      "Document 24 topics : [0] Guns 8.57 - [4] Space 2.07 - [2] Med 1.22 - [3] Hardware 0.61 - [1] Baseball 0.53\n",
      "Document 25 topics : [0] Guns 181.79 - [3] Hardware 54.56 - [2] Med 4.48 - [4] Space 3.83 - [1] Baseball 1.34\n",
      "Document 26 topics : [0] Guns 9.90 - [3] Hardware 8.25 - [1] Baseball 5.85 - [2] Med 5.05 - [4] Space 3.94\n",
      "Document 27 topics : [0] Guns 37.04 - [3] Hardware 17.89 - [1] Baseball 1.48 - [2] Med 0.81 - [4] Space 0.78\n",
      "Document 28 topics : [0] Guns 128.23 - [3] Hardware 34.18 - [1] Baseball 6.78 - [2] Med 2.45 - [4] Space 2.37\n",
      "Document 29 topics : [0] Guns 42.96 - [3] Hardware 13.85 - [4] Space 5.36 - [1] Baseball 2.66 - [2] Med 0.18\n",
      "Document 30 topics : [0] Guns 21.09 - [3] Hardware 9.14 - [1] Baseball 2.15 - [4] Space 0.39 - [2] Med 0.22\n",
      "Document 31 topics : [0] Guns 22.33 - [1] Baseball 2.99 - [3] Hardware 2.83 - [4] Space 2.59 - [2] Med 2.26\n",
      "Document 32 topics : [0] Guns 61.93 - [3] Hardware 39.59 - [1] Baseball 9.55 - [4] Space 6.85 - [2] Med 1.08\n",
      "Document 33 topics : [0] Guns 53.63 - [3] Hardware 15.81 - [1] Baseball 2.21 - [4] Space 0.26\n",
      "Document 34 topics : [0] Guns 40.58 - [3] Hardware 9.67 - [1] Baseball 6.16 - [4] Space 1.51\n",
      "Document 35 topics : [0] Guns 194.13 - [3] Hardware 36.73 - [1] Baseball 7.31 - [4] Space 1.10 - [2] Med 0.74\n",
      "Document 36 topics : [0] Guns 2.18 - [1] Baseball 0.32 - [2] Med 0.28 - [4] Space 0.17\n",
      "Document 37 topics : [3] Hardware 1.58 - [0] Guns 0.93 - [4] Space 0.83 - [1] Baseball 0.65\n",
      "Document 38 topics : [0] Guns 266.31 - [3] Hardware 71.70 - [4] Space 5.65 - [1] Baseball 5.49 - [2] Med 1.84\n",
      "Document 39 topics : \n",
      "Document 40 topics : \n",
      "Document 41 topics : [0] Guns 18.76 - [3] Hardware 15.43 - [1] Baseball 2.11 - [2] Med 0.59 - [4] Space 0.11\n",
      "Document 42 topics : [0] Guns 20.68 - [3] Hardware 8.54 - [1] Baseball 5.21 - [4] Space 0.47 - [2] Med 0.11\n",
      "Document 43 topics : [0] Guns 31.67 - [3] Hardware 7.88 - [1] Baseball 2.59 - [4] Space 0.65 - [2] Med 0.21\n",
      "Document 44 topics : [0] Guns 13.93 - [3] Hardware 5.80 - [4] Space 1.70 - [1] Baseball 0.50\n",
      "Document 45 topics : [0] Guns 26.31 - [4] Space 2.18 - [3] Hardware 2.03 - [1] Baseball 1.42\n",
      "Document 46 topics : [0] Guns 28.99 - [3] Hardware 9.64 - [1] Baseball 4.80 - [4] Space 2.49\n",
      "Document 47 topics : [3] Hardware 9.67 - [0] Guns 7.78 - [4] Space 4.42 - [2] Med 3.01 - [1] Baseball 2.12\n",
      "Document 48 topics : [0] Guns 42.10 - [3] Hardware 24.20 - [4] Space 1.38 - [2] Med 0.87 - [1] Baseball 0.45\n",
      "Document 49 topics : [0] Guns 5.53 - [3] Hardware 1.39\n",
      "Document 50 topics : \n",
      "Document 51 topics : [3] Hardware 8.77 - [0] Guns 4.24 - [1] Baseball 1.00 - [2] Med 0.90\n",
      "Document 52 topics : [0] Guns 5.86 - [3] Hardware 4.46 - [4] Space 0.77 - [2] Med 0.50 - [1] Baseball 0.40\n",
      "Document 53 topics : [0] Guns 85.72 - [3] Hardware 10.17 - [1] Baseball 4.48 - [2] Med 1.58 - [4] Space 1.05\n",
      "Document 54 topics : [1] Baseball 1.86 - [2] Med 0.52 - [4] Space 0.38 - [0] Guns 0.23\n",
      "Document 55 topics : [0] Guns 66.70 - [3] Hardware 24.31 - [4] Space 8.86 - [2] Med 6.14 - [1] Baseball 3.00\n",
      "Document 56 topics : [3] Hardware 0.89 - [0] Guns 0.10\n",
      "Document 57 topics : [0] Guns 122.20 - [3] Hardware 36.01 - [1] Baseball 27.85 - [4] Space 3.99 - [2] Med 2.95\n",
      "Document 58 topics : [0] Guns 18.62 - [1] Baseball 2.58 - [4] Space 1.38 - [3] Hardware 1.30 - [2] Med 0.12\n",
      "Document 59 topics : [3] Hardware 40.32 - [0] Guns 23.65 - [1] Baseball 11.67 - [2] Med 6.85 - [4] Space 3.51\n",
      "Document 60 topics : [0] Guns 17.66 - [3] Hardware 11.55 - [1] Baseball 2.15 - [2] Med 0.33 - [4] Space 0.31\n",
      "Document 61 topics : [0] Guns 5.80 - [3] Hardware 5.27 - [2] Med 3.21 - [1] Baseball 1.51 - [4] Space 0.22\n",
      "Document 62 topics : [0] Guns 47.83 - [3] Hardware 21.17 - [4] Space 3.11 - [1] Baseball 0.77 - [2] Med 0.12\n",
      "Document 63 topics : [0] Guns 35.03 - [3] Hardware 3.79 - [1] Baseball 2.56 - [4] Space 0.56\n",
      "Document 64 topics : [0] Guns 8.53 - [2] Med 1.99 - [3] Hardware 1.92 - [4] Space 0.41 - [1] Baseball 0.14\n",
      "Document 65 topics : [0] Guns 55.58 - [3] Hardware 27.90 - [4] Space 11.71 - [1] Baseball 6.59 - [2] Med 0.22\n",
      "Document 66 topics : [0] Guns 3.11 - [1] Baseball 0.64 - [3] Hardware 0.21\n",
      "Document 67 topics : [0] Guns 70.99 - [3] Hardware 23.70 - [1] Baseball 4.19 - [2] Med 1.43 - [4] Space 0.69\n",
      "Document 68 topics : [0] Guns 26.69 - [3] Hardware 22.14 - [4] Space 11.40 - [1] Baseball 0.62 - [2] Med 0.15\n",
      "Document 69 topics : [0] Guns 24.56 - [3] Hardware 2.60 - [1] Baseball 1.94 - [4] Space 1.71 - [2] Med 0.19\n",
      "Document 70 topics : [0] Guns 43.75 - [3] Hardware 13.11 - [2] Med 4.97 - [1] Baseball 3.72 - [4] Space 1.44\n",
      "Document 71 topics : [0] Guns 14.42 - [3] Hardware 7.50 - [4] Space 5.08 - [2] Med 0.55 - [1] Baseball 0.45\n",
      "Document 72 topics : \n",
      "Document 73 topics : [0] Guns 44.17 - [4] Space 37.85 - [1] Baseball 8.70 - [3] Hardware 8.13 - [2] Med 1.15\n",
      "Document 74 topics : [1] Baseball 34.11 - [0] Guns 32.22 - [4] Space 27.15 - [3] Hardware 6.07 - [2] Med 0.46\n",
      "Document 75 topics : [4] Space 57.86 - [0] Guns 52.08 - [2] Med 13.19 - [3] Hardware 12.05 - [1] Baseball 8.82\n",
      "Document 76 topics : [0] Guns 96.78 - [4] Space 46.40 - [3] Hardware 33.74 - [1] Baseball 6.23 - [2] Med 2.84\n",
      "Document 77 topics : [0] Guns 12.77 - [4] Space 9.69 - [3] Hardware 6.39 - [1] Baseball 2.65 - [2] Med 0.51\n",
      "Document 78 topics : [4] Space 40.96 - [0] Guns 36.45 - [1] Baseball 8.43 - [2] Med 4.03 - [3] Hardware 3.13\n",
      "Document 79 topics : [0] Guns 26.60 - [4] Space 20.69 - [1] Baseball 9.25 - [2] Med 4.60 - [3] Hardware 2.86\n",
      "Document 80 topics : [0] Guns 42.74 - [4] Space 15.30 - [1] Baseball 13.96 - [3] Hardware 10.25 - [2] Med 0.77\n",
      "Document 81 topics : [0] Guns 192.01 - [3] Hardware 84.82 - [4] Space 49.14 - [1] Baseball 2.63 - [2] Med 1.40\n",
      "Document 82 topics : [0] Guns 28.44 - [3] Hardware 25.14 - [1] Baseball 0.74 - [2] Med 0.42 - [4] Space 0.26\n",
      "Document 83 topics : [0] Guns 12.78 - [3] Hardware 6.87 - [1] Baseball 3.01 - [4] Space 2.32\n",
      "Document 84 topics : [4] Space 9.24 - [0] Guns 5.15 - [1] Baseball 1.09 - [2] Med 0.32 - [3] Hardware 0.21\n",
      "Document 85 topics : [0] Guns 118.87 - [4] Space 60.07 - [1] Baseball 15.04 - [3] Hardware 8.71 - [2] Med 1.30\n",
      "Document 86 topics : [4] Space 26.63 - [0] Guns 25.86 - [3] Hardware 6.77 - [1] Baseball 6.14 - [2] Med 0.61\n",
      "Document 87 topics : [4] Space 16.08 - [0] Guns 13.90 - [3] Hardware 7.62 - [1] Baseball 5.99 - [2] Med 0.42\n",
      "Document 88 topics : [0] Guns 39.49 - [3] Hardware 14.70 - [2] Med 5.28 - [4] Space 1.94 - [1] Baseball 1.60\n",
      "Document 89 topics : [0] Guns 31.81 - [4] Space 21.47 - [3] Hardware 14.38 - [1] Baseball 1.93 - [2] Med 0.41\n",
      "Document 90 topics : [0] Guns 7.50 - [3] Hardware 4.98 - [4] Space 3.71 - [1] Baseball 0.46 - [2] Med 0.36\n",
      "Document 91 topics : [0] Guns 17.04 - [3] Hardware 13.69 - [4] Space 13.25 - [1] Baseball 4.86 - [2] Med 1.16\n",
      "Document 92 topics : [0] Guns 15.83 - [1] Baseball 11.50 - [4] Space 10.09 - [3] Hardware 4.35 - [2] Med 0.23\n",
      "Document 93 topics : [0] Guns 1.88 - [4] Space 1.38 - [1] Baseball 0.42 - [3] Hardware 0.32\n",
      "Document 94 topics : [4] Space 9.76 - [0] Guns 6.94 - [2] Med 6.04 - [3] Hardware 4.25 - [1] Baseball 4.01\n",
      "Document 95 topics : [0] Guns 39.50 - [4] Space 16.88 - [1] Baseball 12.25 - [3] Hardware 7.78 - [2] Med 1.59\n",
      "Document 96 topics : [0] Guns 22.33 - [4] Space 8.97 - [1] Baseball 8.90 - [3] Hardware 7.72 - [2] Med 1.07\n",
      "Document 97 topics : \n",
      "Document 98 topics : [0] Guns 8.33 - [4] Space 3.71 - [1] Baseball 3.05 - [3] Hardware 2.50 - [2] Med 1.40\n",
      "Document 99 topics : [0] Guns 145.81 - [4] Space 88.77 - [3] Hardware 53.43 - [1] Baseball 11.33 - [2] Med 1.67\n",
      "Document 100 topics : [4] Space 7.93 - [0] Guns 5.21 - [1] Baseball 4.63 - [3] Hardware 3.60 - [2] Med 1.64\n"
     ]
    }
   ],
   "source": [
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(test_topic_distributions):\n",
    "    \n",
    "    # Sort the topic distribution by probability in descending order\n",
    "    sorted_topic_dist = sorted(enumerate(topic_dist), key=lambda x: -x[1])\n",
    "    \n",
    "    # Create a list to store the formatted topics\n",
    "    formatted_topics = []\n",
    "\n",
    "    # Format and store each topic\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        # Exclude probabilities under 0.2\n",
    "        if probability < 0.1:\n",
    "            continue\n",
    "        \n",
    "        # Associate label to the topic\n",
    "        category_name = topic_category_mapping.get(topic_id, f'Unknown Category {topic_id}')\n",
    "\n",
    "        formatted_topic = f\"[{topic_id}] {category_name} {probability:.2f}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "\n",
    "    # Join the formatted topics into a string\n",
    "    formatted_topics_str = \" - \".join(formatted_topics)\n",
    "\n",
    "    print(f\"Document {i + 1} topics : {formatted_topics_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccbebd-84ec-4a3f-8790-6cbec2cc5316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c2d4c-8d0c-464f-bcf2-196255c8e697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d650780-9459-4073-a9ff-8d0fafd51257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_score(lda_model, texts, dictionary):\n",
    "    topics = lda_model.get_topics()\n",
    "    coherence_score = 0.0\n",
    "    num_topics = len(topics)\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        topic = topics[i]\n",
    "        topic_words = [dictionary[word_id] for word_id in topic]\n",
    "        topic_word_set = set(topic_words)\n",
    "        topic_word_freq = {word: topic_words.count(word) for word in topic_word_set}\n",
    "\n",
    "        for j in range(num_texts):\n",
    "            text = texts[j]\n",
    "            text_word_set = set(text)\n",
    "            common_words = topic_word_set.intersection(text_word_set)\n",
    "            common_word_freq = {word: text.count(word) for word in common_words}\n",
    "\n",
    "            coherence_score += np.log(sum(common_word_freq.values()) + 1) - np.log(len(text))\n",
    "\n",
    "    coherence_score /= num_topics * num_texts\n",
    "\n",
    "    return coherence_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434b103-6949-49b2-9802-d1223f285c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "def compute_coherence_score(topic_word_counts, lemmatized_corpus, dictionary, top_n=10):\n",
    "    # Get top N words for each topic\n",
    "    top_words = [[dictionary[i] for i in np.argsort(topic_word_counts[t])[-top_n:]] for t in range(len(topic_word_counts))]\n",
    "\n",
    "    # Compute word co-occurrence matrix\n",
    "    co_occurrences = defaultdict(int)\n",
    "    for doc in lemmatized_corpus:\n",
    "        for w1, w2 in itertools.combinations(set(doc), 2):\n",
    "            if w1 != w2:\n",
    "                co_occurrences[(w1, w2)] += 1\n",
    "                co_occurrences[(w2, w1)] += 1\n",
    "\n",
    "    # Compute coherence score\n",
    "    coherence_score = 0\n",
    "    for topic in top_words:\n",
    "        topic_score = 0\n",
    "        for i, word_i in enumerate(topic[:-1]):\n",
    "            for word_j in topic[i+1:]:\n",
    "                if (word_i, word_j) in co_occurrences:\n",
    "                    topic_score += log((co_occurrences[(word_i, word_j)] + 1) / co_occurrences[word_i])\n",
    "        coherence_score += topic_score / (top_n * (top_n - 1) / 2)\n",
    "\n",
    "    return coherence_score / len(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936061a-978f-4b83-821e-a6af5b97fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_score = compute_coherence_score(topic_word_counts, lemmatized_train_data, dictionary, top_n=10)\n",
    "print(\"Coherence Score:\", coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e8145-469a-4c6e-b6af-1227f3e2214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f186f08-494f-48a6-bae9-25f3227d6332",
   "metadata": {},
   "source": [
    "# **4. Sentiment Analysis:**\n",
    "Determine the sentiment of the content using the VADER sentiment analyzer from the `vaderSentiment`library.\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained sentiment analysis tool specifically designed for social media texts and doesn't require preprocessing like tokenization, stemming, or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996cfb1-d1fd-467e-909c-26e7c93ca8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to analyze sentiment using VADER\n",
    "def get_sentiment(text):\n",
    "     # Initialize VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c134384-52d4-4cd5-9f16-471274289860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sentiments = [get_sentiment(text) for text in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64681b6-d2b8-45c3-a847-d77cbc338ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the sentiment scores for the first 10 test documents\n",
    "scores_list = []\n",
    "for i, sentiment in enumerate(test_sentiments):\n",
    "    scores_list.append(sentiment['compound'])\n",
    "    print(f\"Document {i + 1}: {sentiment}\")\n",
    "    \n",
    "print(f\"\\nAverage sentiment: {np.mean(scores_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d695c42-a3fb-4286-80b4-11ed8777db26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebc9b40b-c510-49bd-b001-4b5bb262c9fc",
   "metadata": {},
   "source": [
    "# **5. Summarization:**\n",
    "Generate summaries of the relevant content using extractive summarization based on word frequency. For this, I'll follow these steps:\n",
    "- 1. Split the text into sentences.\n",
    "- 2. Tokenize the sentences.\n",
    "- 3. Calculate the frequency of each word in the text.\n",
    "- 4. Assign a score to each sentence based on the frequency of the words in the sentence.\n",
    "- 5. Select the top N sentences with the highest scores as the summary.\n",
    "\n",
    "This is a simple implementation of extractive summarization without using any libraries. Note that this approach does not consider the semantic meaning of words or the coherence of the summary. More advanced techniques, such as using word embeddings or graph-based methods, can improve the quality of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22165272-7b33-4c13-aed1-10fd61c61d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractive_summarization(text, n_sentences=3):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.strip().split('.')\n",
    "\n",
    "    # Tokenize and preprocess the text\n",
    "    word_freq = {}\n",
    "    for sentence in sentences:\n",
    "        stemmed_tokens, _ = preprocess_text([sentence])\n",
    "        # Flatten the stemmed_tokens list\n",
    "        stemmed_tokens = [token for sublist in stemmed_tokens for token in sublist]\n",
    "        for token in stemmed_tokens:\n",
    "            if token not in word_freq:\n",
    "                word_freq[token] = 1\n",
    "            else:\n",
    "                word_freq[token] += 1\n",
    "\n",
    "    # Calculate the score for each sentence\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        stemmed_tokens, _ = preprocess_text([sentence])\n",
    "        # Flatten the stemmed_tokens list\n",
    "        stemmed_tokens = [token for sublist in stemmed_tokens for token in sublist]\n",
    "        for token in stemmed_tokens:\n",
    "            if token in word_freq:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_freq[token]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freq[token]\n",
    "\n",
    "    # Select the top N sentences with the highest scores\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n_sentences]\n",
    "    summary = '. '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791a142-a079-4db3-b720-e47f5a2d6733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries = [extractive_summarization(text, n_sentences=1) for text in test_data]\n",
    "\n",
    "# Print summary for every document in the test set\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Summary {i + 1}: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044b883-3b05-4b3f-a414-ecc8e4509ec9",
   "metadata": {},
   "source": [
    "# **6. Visualization and Reporting:**\n",
    "Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d055d9d-8caf-4b2f-b490-309d27aaa9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge all the texts in the test set\n",
    "merged_test_text = ' '.join(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46818890-8183-444f-8d75-b3432b1d897e",
   "metadata": {},
   "source": [
    "## Main topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c6c23-4111-40f8-b55a-12b7805aac95",
   "metadata": {},
   "source": [
    "### Gensim version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61c928a2-a91d-4550-9c60-55ebb242ab06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the topic distribution for the merged text\n",
    "merged_text_topic_distribution = get_topic_distribution(lda_model, dictionary, merged_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cb2e3c4-306d-44bb-9d91-3c915b24bf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main topics distribution:\n",
      "[0] Guns -> 0.6212282776832581\n",
      "[1] Baseball -> 0.25317147374153137\n",
      "\n",
      "Main Guns keywords:\n",
      "[('would', 0.009276755), ('one', 0.008098404), ('gun', 0.0074029597), ('like', 0.005558507), ('get', 0.005383595), ('know', 0.005368865), ('time', 0.005036473), ('people', 0.0050030183), ('problem', 0.004014722), ('thing', 0.0036522488)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMain topics distribution:\")\n",
    "\n",
    "# Display the topic distribution for all test documents\n",
    "for i, topic_dist in enumerate(merged_text_topic_distribution):\n",
    "    sorted_topic_dist = sorted([topic_dist], key=lambda x: -x[1])\n",
    "    formatted_topics = []\n",
    "    for topic_id, probability in sorted_topic_dist:\n",
    "        topic_name = topic_category_mapping.get(topic_id, f'Unknown Category {topic_id}')\n",
    "        formatted_topic = f\"[{topic_id}] {topic_name} -> {probability}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "    formatted_topics_str = \" - \".join(formatted_topics)\n",
    "    print(f\"{formatted_topics_str}\")\n",
    "\n",
    "# Identify the main topic based on the highest average topic distribution\n",
    "main_topic = max(merged_text_topic_distribution, key=lambda x: x[1])[0]\n",
    "topic_name = topic_category_mapping.get(main_topic, f'Unknown Category {topic_num}')\n",
    "\n",
    "# Display the main topic keywords\n",
    "main_topic_keywords = lda_model.show_topic(main_topic)\n",
    "print(f\"\\nMain {topic_name} keywords:\\n{main_topic_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67b287-de9a-4970-918d-01b97bc88173",
   "metadata": {},
   "source": [
    "### Scratch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6a676a4-4d85-432f-b828-aa83170afd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the topic distribution for the document\n",
    "topic_distribution = infer_topic_distribution_scratch(merged_test_text, dictionary, topic_word_counts, doc_topic_counts, alpha=0.1, beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "589e7b11-a1d1-4a59-b941-bbee09ba06c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main topics distribution:\n",
      "[0] Guns -> 5045.698170268956 - [3] Hardware -> 2469.2684302414887 - [4] Space -> 785.5779189439983 - [1] Baseball -> 93.53890919586722 - [2] Med -> 68.9165713496919\n",
      "\n",
      "Main Guns keywords:\n",
      "['would', 'one', 'year', 'get', 'like', 'think', 'people', 'good', 'know', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMain topics distribution:\")\n",
    "\n",
    "# Display the topic distribution\n",
    "sorted_topic_dist = sorted(enumerate(topic_distribution), key=lambda x: -x[1])\n",
    "formatted_topics = []\n",
    "for topic_id, probability in sorted_topic_dist:\n",
    "    topic_name = topic_category_mapping.get(topic_id, f'Unknown Category {topic_id}')\n",
    "    formatted_topic = f\"[{topic_id}] {topic_name} -> {probability}\"\n",
    "    formatted_topics.append(formatted_topic)\n",
    "formatted_topics_str = \" - \".join(formatted_topics)\n",
    "print(f\"{formatted_topics_str}\")\n",
    "\n",
    "# Identify the main topic based on the highest probability\n",
    "main_topic = sorted_topic_dist[0][0]\n",
    "topic_name = topic_category_mapping.get(main_topic, f'Unknown Category {main_topic}')\n",
    "\n",
    "# Display the main topic keywords\n",
    "main_topic_keywords = []\n",
    "for word, _ in sorted(enumerate(topic_word_counts[main_topic]), key=lambda x: -x[1])[:10]:\n",
    "    main_topic_keywords.append(dictionary[word])\n",
    "print(f\"\\nMain {topic_name} keywords:\\n{main_topic_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d00bc-bec8-4128-8366-861f1dcf1c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cabd4c72-8148-4b4e-bf1b-3dece9cc4867",
   "metadata": {},
   "source": [
    "## Average sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b542c4c-9b6f-4317-86db-df392c21ed7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_text_sentiment = get_sentiment(merged_test_text)\n",
    "\n",
    "print(f\"Global Text Sentiment: {merged_text_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cf406-345f-40d0-9f83-33e1618ec4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to visualize sentiment scores\n",
    "def visualize_sentiment(sentiment_scores):\n",
    "    labels = ['Positive', 'Neutral', 'Negative']\n",
    "    values = [sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg']]\n",
    "\n",
    "    plt.bar(labels, values)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Sentiment Analysis')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_sentiment(merged_text_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6db6d-4211-4af8-bc32-cca1494e34ed",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1f6ef-e80b-41b9-8acb-7b7f956dec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = extractive_summarization(merged_test_text, n_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465cbe7-7e5f-4906-bad7-d85e43e0ab03",
   "metadata": {},
   "source": [
    "## Word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905634a2-65a2-4901-8503-238999bc3fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_wordcloud(texts):\n",
    "    all_text = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=5, max_words=100).generate(all_text)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Texts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed98675-3a29-4103-960b-b83ecf221bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate word cloud\n",
    "generate_wordcloud([merged_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb9713-1938-4169-b8e1-ae7e36e9b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74377c-6310-4b3a-b2db-b68be4cbb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e6704-9bee-447a-ba5e-54bccebe7027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b273e-8131-44c9-bd52-7cbf6000f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721a5f2-71ba-4394-ba63-ce81ae1683fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd15b3-f2f2-4d6a-80e6-8d07a088efab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b5b29-b75c-45b0-b8c9-46be8762f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b7f83-085f-43c2-b664-632e0afb7915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ed18c-26f9-4993-a7f4-982cf7d6bc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
