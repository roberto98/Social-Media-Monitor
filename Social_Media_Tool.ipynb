{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYtWa-kq9UnS"
   },
   "source": [
    "# Social Media Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e55JJEYn9Y_Z"
   },
   "source": [
    "In my project i'll implement, using Python, a social media monitor that\n",
    "tracks topics or trends from social media or blogs.\n",
    "I'll use the SKlearn 20 newsgroups dataset that comprises around 18000\n",
    "newsgroups posts on 20 topics.\n",
    "Then i'll pre-process the text using the nltk library, i'll apply\n",
    "stopword removal, stemming and lemmatization.\n",
    "After the text is pre-processed i'll apply a model to identify the main\n",
    "topic, the idea is to implement from scratch the Latent Dirichlet\n",
    "Allocation model and made a comparison between my version and the\n",
    "version taken from gensim or sklearn libraries.\n",
    "I'll perform a sentiment analysis using the Vader library.\n",
    "I'll generate a summary of the relevant content using extractive\n",
    "summarization based on word frequencies, to do that i'll not use any\n",
    "library.\n",
    "Finally i'll visualize the results, showing the distribution of topics,\n",
    "sentiment scores and summaries of relevant content. In this phase i'll\n",
    "use matplotlib and wordcloud libraries.\n",
    "\n",
    "About the LDA i found the following interesting papers:\n",
    "\n",
    "https://arxiv.org/pdf/1711.04305.pdf\n",
    "\n",
    "https://ai.stanford.edu/~ang/papers/jair03-lda.pdf\n",
    "\n",
    "\n",
    "## IDEA\n",
    "- Creo 2 datasets bilanciati (tipo 100 tweets e 100 tweets)-> covid e champions\n",
    "- Traino LDA su entrambi\n",
    "- Creo nuovo dataset misto (70-30%)\n",
    "- Identifico topic del dataset misto\n",
    "- Sentument\n",
    "- Riassunto\n",
    "- Plotto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJYrgmbDI5oY"
   },
   "source": [
    "I creating a custom media monitor that tracks specific topics or trends across various platforms, such as news articles, blog posts, and social media. This project can help businesses or individuals stay up-to-date with the latest developments and discussions related to their areas of interest.\n",
    "\n",
    "To implement this project, i'll follow these steps:\n",
    "\n",
    "1. **Data Collection**: Gather data from various sources like news websites, blogs, and social media using APIs or web scraping techniques or RSS feed.\n",
    "2. **Text Preprocessing**: Clean and normalize the text data, as mentioned in the previous answer (tokenization, stopword removal, stemming/lemmatization).\n",
    "3. **Topic Modeling**: Employ topic modeling techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF) to identify the main topics or themes present in the collected data. This will help you filter relevant content based on the topics of interest.\n",
    "4. **Sentiment Analysis**: Determine the sentiment of the content (positive, negative, or neutral) using techniques like rule-based approaches (e.g., VADER sentiment analyzer) or pre-trained models (e.g., TextBlob).\n",
    "5. **Summarization**: Generate summaries of the relevant content using extractive or abstractive summarization techniques, so that users can quickly grasp the main points without reading the entire text.\n",
    "6. **Visualization and Reporting**: Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install pyLDAvis\n",
    "%pip install vaderSentiment\n",
    "%pip install wordcloud\n",
    "%pip install matplotlib\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Collection:** \n",
    "Gather data from various sources using APIs or web scraping techniques. For example, you can use the requests library for accessing APIs and BeautifulSoup for web scraping. In this case I'll use the `20newsgroups` module of `sklearn.datasets`\n",
    "\n",
    "https://imerit.net/blog/top-25-twitter-datasets-for-natural-language-processing-and-machine-learning-all-pbm/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxiing https://www.kaggle.com/datasets/bwandowando/boxing-twitter-dataset\n",
    "space race https://www.kaggle.com/datasets/amartyanambiar/the-space-race-tweets\n",
    "crypto https://www.kaggle.com/datasets/gauravduttakiit/bitcoin-tweets-16m-tweets-with-sentiment-tagged\n",
    "airline (pos) https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "avengers https://www.kaggle.com/datasets/kavita5/twitter-dataset-avengersendgame\n",
    "covid (neg) https://data.gesis.org/tweetscov19/\n",
    "\n",
    "\n",
    "\n",
    "topic modeling lda https://www.kaggle.com/code/datajameson/topic-modelling-nlp-amazon-reviews-bbc-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter dataset\n",
    "filter the negative tweets from the TweetsCOV19 dataset \n",
    "https://data.gesis.org/tweetscov19/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "tweetscov19_df = pd.read_csv(\"TweetsCOV19.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Assign column names\n",
    "tweetscov19_df.columns = [\"tweet_id\", \"timestamp\", \"entities\", \"sentiment\", \"mentions\", \"hashtags\", \"urls\"]\n",
    "\n",
    "# Filter the negative tweets\n",
    "negative_tweets_df = tweetscov19_df[tweetscov19_df[\"sentiment\"].apply(lambda x: int(x.split()[1]) < 0)]\n",
    "\n",
    "# Save the negative tweets to a text file\n",
    "negative_tweets_df.to_csv(\"negative_tweets.txt\", columns=[\"tweet_id\", \"sentiment\"], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple dataset\n",
    " filter the positive tweets from the Apple Twitter Sentiment dataset: \n",
    " https://www.kaggle.com/seriousran/appletwittersentimenttexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "apple_sentiment_df = pd.read_csv(\"apple-twitter-sentiment-texts.csv\")\n",
    "\n",
    "# Filter the positive tweets\n",
    "positive_apple_tweets_df = apple_sentiment_df[apple_sentiment_df[\"sentiment\"] == \"positive\"]\n",
    "\n",
    "# Save the positive tweets to a text file\n",
    "positive_apple_tweets_df.to_csv(\"positive_apple_tweets.txt\", columns=[\"text\", \"sentiment\"], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock dataset\n",
    "https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:42:38.523389300Z",
     "start_time": "2023-06-20T17:42:36.130544700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size is:  18846\n",
      "\n",
      "The topics are: \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:57:32.103175500Z",
     "start_time": "2023-06-20T17:57:29.368476700Z"
    },
    "id": "dY4t_hi0socR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  11314\n",
      "Test dataset size:  3844\n"
     ]
    }
   ],
   "source": [
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "print(\"The total size is: \", len(newsgroups_all.data))\n",
    "print(\"\\nThe topics are: \\n\",newsgroups_all.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the 20newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Select 10 categories for the test set\n",
    "selected_categories = ['alt.atheism', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'rec.autos', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns']\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=selected_categories)\n",
    "\n",
    "print(\"Train dataset size: \", len(newsgroups_train.data))\n",
    "print(\"Test dataset size: \", len(newsgroups_test.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Text Preprocessing:** \n",
    "Clean and normalize the text data using tokenization, stopword removal, and stemming/lemmatization. I'll use the `nltk` library for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T18:10:14.389848200Z",
     "start_time": "2023-06-20T18:10:14.371141500Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1685128657693,
     "user": {
      "displayName": "alessandra danovaro",
      "userId": "08179344108066784358"
     },
     "user_tz": -120
    },
    "id": "fklQYkddJ6Sw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # Lemmatize\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemmatized_texts = [[lemmatizer.lemmatize(word) for word in text] for text in stemmed_tokens]\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Topic Modeling:** \n",
    "Apply Latent Dirichlet Allocation (LDA) to identify the main topics in the collected data. You can use the `gensim library` to perform LDA.\n",
    "\n",
    "Evaluate both models on the testing set using perplexity and coherence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of LDA is that it is a generative probabilistic model, where each document is viewed as a mixture of various topics, and each topic is characterized as a distribution over words.\n",
    "\n",
    "To understand how the LDA model works in the Gensim library, let's break it down step-by-step:\n",
    "\n",
    "Preparing inputs: The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. The dictionary is a mapping of (word_id, word_frequency) and the corpus is a list of such mappings for each document\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "Building the LDA Model: Once we have the inputs ready, we can build the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Other parameters include alpha and eta (which affect sparsity of the topics), chunksize (the number of documents to be used in each training chunk), update_every (how often the model parameters should be updated), and passes (the total number of training passes)\n",
    "\n",
    "Understanding the Output: The LDA model will output the word distribution for each topic and the topic contribution for each document. The topics are just a collection of dominant keywords that are typical representatives. By looking at the keywords, you can identify what the topic is all about\n",
    "\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Latent Dirichlet Allocation in a nutshell:\n",
    "\n",
    "The order of words is not important in a document - Bag of Words model.\n",
    "A document is a distribution over topics\n",
    "Each topic, in turn, is a distribution over words belonging to the vocabulary\n",
    "LDA is a probabilistic generative model. It is used to infer hidden variables using a posterior distribution.\n",
    "Imagine the process of creating a document to be something like this -\n",
    "\n",
    "Choose a distribution over topics\n",
    "Draw a topic - and choose word from the topic. Repeat this for each of the topics\n",
    "LDA is sort of backtracking along this line -given that you have a bag of words representing a document, what could be the topics it is representing ?\n",
    "\n",
    "http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "\n",
    "https://www.youtube.com/watch?v=DDq3OVp9dNA\n",
    "\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a generative statistical model that is used for topic modeling. It's an unsupervised learning method, meaning that it generates a probabilistic model to identify groups of topics without the need for known class labels. It uses only the distribution of words to mathematically model topic.\n",
    "\n",
    "The type of datasets that should be used for training in an LDA model are text documents. These can be a corpus of documents where each document is a collection of words. The LDA algorithm finds the weight of connections between documents and topics and between topics and words.\n",
    "\n",
    "\n",
    "https://www.baeldung.com/cs/latent-dirichlet-allocation\n",
    "\n",
    "The documents can come from any domain as long as they contain text. For example, they could be customer reviews, news articles, research papers, social media posts, etc. The words in the documents are collected into n-grams (a contiguous sequence of n items from a given sample of text or speech) and used to create a dictionary. This dictionary is then used to train the LDA model. \n",
    "\n",
    "It's important to note that the text in the documents should be preprocessed before being used for training the LDA model. This preprocessing can include removing stop words (commonly used words such as 'the', 'a', 'an', 'in'), lowercasing all the words, and lemmatizing the words (reducing inflectional forms and sometimes derivationally related forms of a word to a common base form)\n",
    "\n",
    "When configuring the LDA model, some parameters that can be set include the rho parameter (a prior probability for the sparsity of topic distributions), the alpha parameter (a prior probability for the sparsity of per-document topic weights), the estimated number of documents, the size of the batch, the initial value of iteration used in learning update schedule, the power applied to the iteration during updates, and the number of passes over the data\n",
    "\n",
    "\n",
    "As a result of the training, each document will be represented as a combination of topics, and each topic will be represented as a distribution over words. This can be used to classify new documents, identify related terms, and create recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "\n",
    "1.2 Words that have fewer than 3 characters are removed.\n",
    "\n",
    "1.3 Remove all stop words.\n",
    "\n",
    "1.4 Lemmatized the words.\n",
    "\"\"\"\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return SnowballStemmer('english').stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of your task is printing the most frequent or relevant topics from your texts. You can achieve this by applying your LDA model to a new, unseen document, and then using the output to find the most relevant topic. Here is an example of how you can achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'The unseen document text goes here'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "# print the most probable topics\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, for topic modeling, you can train your model on any similar corpus of text documents. It doesn't necessarily have to contain the same topics as your unseen documents but having some overlap would likely improve performance. For example, if you're looking to categorize social media posts from a specific platform or about a specific subject, you would ideally use a training set gathered from the same or similar platform/subject.\n",
    "\n",
    "However, if you want to train an LDA model on the specific topics you mentioned, you would need a dataset that contains a substantial number of documents related to these topics.\n",
    "\n",
    "\n",
    "Find suitable datasets: For each of your topics, you can find relevant datasets as follows:\n",
    "Covid: LitCovid is a literature hub in PubMed (an online database of biomedical references) specifically catering to Covid-19 related research papers.\n",
    "Champions League: TheSportsDB is a free and community-driven sports database that includes data about teams, players, matches, and more.\n",
    "Airline Use: Bureau of Transportation Statistics provides data about the performance and services of US airlines.\n",
    "Avengers Endgame: IMDb dataset could be used for movie reviews. It's a popular movie database which provides user reviews, but please do keep in mind it includes reviews for all the movies so you would need to filter Avengers Endgame related reviews.\n",
    "Space: You can use NASA Datasets which features a variety of datasets from satellite sightseeing to solar dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENSIM\n",
    "https://www.kaggle.com/code/datajameson/topic-modelling-nlp-amazon-reviews-bbc-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('actual', 1),\n",
       "  ('also', 1),\n",
       "  ('anyway', 1),\n",
       "  ('basher', 1),\n",
       "  ('beat', 1),\n",
       "  ('better', 1),\n",
       "  ('bit', 3),\n",
       "  ('bowman', 1),\n",
       "  ('confus', 1),\n",
       "  ('coupl', 1),\n",
       "  ('devil', 2),\n",
       "  ('disappoint', 1),\n",
       "  ('end', 1),\n",
       "  ('fan', 1),\n",
       "  ('final', 1),\n",
       "  ('fo', 1),\n",
       "  ('fun', 2),\n",
       "  ('game', 2),\n",
       "  ('go', 2),\n",
       "  ('howev', 1),\n",
       "  ('island', 1),\n",
       "  ('jagr', 2),\n",
       "  ('jersey', 1),\n",
       "  ('kill', 1),\n",
       "  ('kind', 1),\n",
       "  ('lack', 1),\n",
       "  ('let', 1),\n",
       "  ('lose', 1),\n",
       "  ('lot', 2),\n",
       "  ('man', 1),\n",
       "  ('massacr', 1),\n",
       "  ('much', 1),\n",
       "  ('next', 1),\n",
       "  ('pen', 5),\n",
       "  ('playoff', 1),\n",
       "  ('post', 1),\n",
       "  ('prais', 1),\n",
       "  ('pretti', 1),\n",
       "  ('pulp', 1),\n",
       "  ('put', 1),\n",
       "  ('puzzl', 1),\n",
       "  ('recent', 1),\n",
       "  ('regular', 2),\n",
       "  ('relief', 1),\n",
       "  ('reliev', 1),\n",
       "  ('rule', 1),\n",
       "  ('season', 2),\n",
       "  ('see', 1),\n",
       "  ('show', 1),\n",
       "  ('sinc', 1),\n",
       "  ('stat', 1),\n",
       "  ('sure', 1),\n",
       "  ('thought', 1),\n",
       "  ('watch', 1),\n",
       "  ('wors', 1)]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the 20 newsgroups documents\n",
    "processed_docs = [preprocess_text(doc) for doc in newsgroups_all.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(docs) for docs in processed_docs]\n",
    "\n",
    "\n",
    "# human-readable format of corpus (term-frequency)\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Topic Model**\n",
    "Let's now build the topic model. We'll define 10 topics to start with. The hyperparameter alpha affects sparsity of the document-topic (theta) distributions, whose default value is 1. Similarly, the hyperparameter eta can also be specified, which affects the topic-word distribution's sparsity.\n",
    "\n",
    "https://www.kaggle.com/code/datajameson/topic-modelling-nlp-amazon-reviews-bbc-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "example1 = \"Travel: A passport to joy, discovery, and unforgettable memories. From ancient wonders to vibrant cultures, every step is a breathtaking adventure that enriches our souls. Let's embrace the world's beauty, forge connections, and let wanderlust guide us. #Travel #Wanderlust\"\n",
    "example2 = \"Traffic: A daily nightmare of frustration and stress. Endless queues, impatient drivers, and never-ending roadworks make our commute an exasperating odyssey. Time slips away, stress soars, and productivity suffers. We long for a smoother journey. #TrafficJam #CommuterWoes\"\n",
    "example3 = \"Navigating through the maze of concrete and steel, the daily commute weaves a tale of frustration and stress, as we find ourselves trapped in an unending cycle of traffic. The promise of a productive day slips away as we inch forward at a snail's pace, surrounded by an endless sea of brake lights. The once simple act of reaching our destination has become a Herculean task, where every minute spent behind the wheel feels like an eternity. Gridlock traffic, impatient drivers, and never-ending road construction all conspire to transform our once serene journey into a maddening odyssey of chaos. The symphony of car horns blares in our ears, serving as a constant reminder of the exasperation that envelops us. The mere thought of being confined to our vehicles for hours on end fills us with a sense of despair, as we watch precious moments of our lives slip away, wasted in a sea of congestion. As we inch forward, our stress levels skyrocket, and our patience wears thin. The glaring red numbers on the clock mock us, reminding us of the valuable time slipping away with each passing moment. The promise of a relaxing evening or a well-deserved rest becomes an unattainable dream, overshadowed by the overwhelming burden of the daily commute. The consequences of this traffic nightmare extend beyond our personal lives. It hinders productivity, damages the environment, and takes a toll on our mental and physical well-being. The once vibrant city streets have transformed into battlegrounds of frustration and anger, as we fight for every inch of progress, caught in a relentless cycle of congestion. In this congested world, we yearn for a solutionâ€”a respite from the unrelenting grip of traffic. A transformative change that eases our burdens and frees us from this daily ordeal. Until then, we brace ourselves for another arduous journey, steeling our nerves and hoping that one day\"\n",
    "example4 = \"Work Struggles: Battling Stress in the 9 to 5 Trenches. Endless deadlines, overwhelming tasks, and the constant race against the clock. The daily grind takes a toll on our well-being. Let's find balance, prioritize self-care, and strive for a healthier work-life equilibrium. #WorkStress #SelfCareNeeded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.008*\"would\" + 0.007*\"one\" + 0.005*\"use\" + 0.005*\"like\" + 0.005*\"peopl\" + '\n",
      "  '0.005*\"get\" + 0.004*\"know\" + 0.004*\"think\" + 0.004*\"time\" + 0.003*\"say\"'),\n",
      " (1,\n",
      "  '0.013*\"1\" + 0.011*\"2\" + 0.010*\"0\" + 0.010*\"x\" + 0.007*\"use\" + 0.007*\"3\" + '\n",
      "  '0.007*\"max\" + 0.007*\"file\" + 0.006*\"4\" + 0.006*\"7\"')]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess two new unseen documents\n",
    "unseen_docs  = [example1, example2, example4, example3]\n",
    "preprocessed_unseen_docs = [preprocess_text(doc) for doc in unseen_docs]\n",
    "print(len(preprocessed_unseen_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics()\n",
    "pprint(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the model using coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence score\n",
    "coherence_model_lda=CoherenceModel(model=lda_model,texts=processed_docs,dictionary=dictionary,coherence='c_v')\n",
    "coherence_lda=coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:',coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main topic: 0\n",
      "Probability: 0.9319106638431549\n",
      "Ratio: 0.9327861873172403\n",
      "Keywords: [('would', 0.0075205686), ('one', 0.0073870406), ('use', 0.005440685), ('like', 0.005001823), ('peopl', 0.0047082114), ('get', 0.004608978), ('know', 0.0043750065), ('think', 0.0038991002), ('time', 0.003843012), ('say', 0.0034865953)]\n"
     ]
    }
   ],
   "source": [
    "# Get the topic distribution for each unseen document\n",
    "unseen_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_unseen_docs]\n",
    "\n",
    "unseen_doc_topics = [lda_model.get_document_topics(bow) for bow in unseen_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "num_topics = 4\n",
    "\n",
    "# Calculate the average topic distribution for each topic across the 10 documents\n",
    "avg_topic_distributions = [0] * num_topics\n",
    "for doc_topics in unseen_doc_topics:\n",
    "    for topic_id, topic_prob in doc_topics:\n",
    "        avg_topic_distributions[topic_id] += topic_prob\n",
    "avg_topic_distributions = [prob / len(unseen_doc_topics) for prob in avg_topic_distributions]\n",
    "\n",
    "\n",
    "# Identify the main topic based on the highest average topic distribution\n",
    "main_topic = max(enumerate(avg_topic_distributions), key=lambda x: x[1])[0]\n",
    "main_topic_prob = max(avg_topic_distributions)\n",
    "\n",
    "# Calculate the ratio of the main topic\n",
    "total_prob = sum(avg_topic_distributions)\n",
    "main_topic_ratio = main_topic_prob / total_prob\n",
    "\n",
    "print(f\"Main topic: {main_topic}\")\n",
    "print(f\"Probability: {main_topic_prob}\")\n",
    "print(f\"Ratio: {main_topic_ratio}\")\n",
    "\n",
    "# Display the main topic keywords\n",
    "main_topic_keywords = lda_model.show_topic(main_topic)\n",
    "print(f\"Keywords: {main_topic_keywords}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main topic for unseen Document 1: (0, 0.9417257)\n",
      "Main topic for unseen Document 2: (0, 0.91111666)\n",
      "Main topic for unseen Document 3: (0, 0.87855476)\n"
     ]
    }
   ],
   "source": [
    "# Returns the topics that a particular word belongs to along with its probability in that topic.\n",
    "word_id = dictionary.token2id['space']\n",
    "topics = lda_model.get_term_topics(word_id)\n",
    "print(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T18:22:27.547561200Z",
     "start_time": "2023-06-20T18:22:03.265859400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1221620018028921602268070925\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1221620018028921602268070925_data = {\"mdsDat\": {\"x\": [0.2055776093392409, -0.2055776093392409], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [73.57105062634129, 26.42894937365871]}, \"tinfo\": {\"Term\": [\"1\", \"0\", \"x\", \"2\", \"3\", \"max\", \"file\", \"4\", \"7\", \"r\", \"p\", \"g\", \"window\", \"5\", \"n\", \"imag\", \"q\", \"c\", \"would\", \"8\", \"6\", \"w\", \"9\", \"b\", \"use\", \"program\", \"e\", \"softwar\", \"f\", \"h\", \"peopl\", \"think\", \"god\", \"say\", \"even\", \"believ\", \"said\", \"thing\", \"state\", \"take\", \"govern\", \"mean\", \"person\", \"law\", \"reason\", \"realli\", \"live\", \"fact\", \"car\", \"claim\", \"made\", \"armenian\", \"might\", \"never\", \"jesu\", \"kill\", \"happen\", \"talk\", \"word\", \"cours\", \"would\", \"good\", \"could\", \"well\", \"right\", \"year\", \"go\", \"one\", \"someth\", \"seem\", \"day\", \"way\", \"christian\", \"like\", \"time\", \"come\", \"much\", \"mani\", \"know\", \"see\", \"make\", \"us\", \"want\", \"get\", \"may\", \"also\", \"use\", \"first\", \"two\", \"work\", \"new\", \"0\", \"x\", \"max\", \"g\", \"r\", \"p\", \"w\", \"n\", \"h\", \"z\", \"v\", \"k\", \"u\", \"j\", \"b8f\", \"disk\", \"do\", \"a86\", \"pc\", \"mac\", \"145\", \"ftp\", \"server\", \"widget\", \"font\", \"scsi\", \"1d9\", \"interfac\", \"printer\", \"unix\", \"f\", \"graphic\", \"e\", \"directori\", \"output\", \"7\", \"l\", \"softwar\", \"9\", \"c\", \"8\", \"window\", \"1\", \"imag\", \"file\", \"4\", \"2\", \"3\", \"5\", \"6\", \"b\", \"q\", \"25\", \"use\", \"program\", \"drive\", \"system\"], \"Freq\": [7138.0, 5030.0, 4922.0, 6191.0, 4150.0, 3258.0, 3559.0, 3511.0, 3088.0, 2905.0, 2890.0, 2815.0, 2821.0, 3049.0, 2389.0, 2654.0, 3672.0, 2433.0, 10286.0, 2302.0, 2520.0, 1970.0, 2025.0, 2161.0, 11105.0, 3157.0, 1707.0, 1720.0, 1678.0, 1625.0, 6434.019012619355, 5328.326170681305, 4188.169208305729, 4764.6164469183, 3656.362929549747, 2817.213554935002, 2641.1114383970375, 3583.6089479470174, 2620.9063474597588, 2978.7101489876527, 2000.0220994069682, 2628.3013795616157, 2402.4993146798606, 1862.1390500129255, 2177.877998620496, 2236.0519117680365, 1701.183672275792, 1787.8800988790974, 1542.8736046193244, 1557.8267337035225, 1864.379322839239, 1371.3700767986884, 1979.088386857382, 1799.632383675167, 1229.2035118539495, 1267.2502870314675, 1488.237960746747, 1352.8210886963707, 1727.9248808859215, 1430.5747190785744, 10277.253658852651, 3748.7209688034463, 4224.92228240218, 3720.794716003343, 3535.119656911013, 3910.1727001280033, 4375.780046389011, 10094.780716749954, 2482.8067797219715, 2510.54880897905, 2226.370153234889, 3525.1894034087773, 1942.4719690851605, 6835.254707572935, 5251.678676995689, 2945.2667351581113, 3237.301140949379, 3369.768686346792, 5978.6769585320935, 3685.1638464599964, 4651.932728259609, 3112.637483012694, 3660.7916126159957, 6298.411394430735, 3623.234191167617, 4645.161638071173, 7434.982838487928, 3152.5051758702275, 3094.487486832574, 3435.787531578637, 3124.683920944387, 5029.89206460738, 4921.522711710313, 3258.0678663524504, 2815.4424815701936, 2904.6517934550334, 2889.7354171530164, 1969.581396873223, 2388.6691184066367, 1625.2850655068062, 1368.8191521698106, 1598.4114940961394, 1598.7612466821458, 1439.9457981651992, 1352.5396551682145, 862.7184735530924, 1333.705821306465, 871.8895921388016, 730.8864941956509, 874.1126927263257, 952.9023493021749, 631.0369885656446, 886.7619636219912, 855.0212306525309, 537.1419064141658, 575.4182122162543, 513.3134113172532, 468.19927101139444, 519.8514415618422, 514.430619373436, 523.7938161358719, 1668.7802285236169, 1040.867810691193, 1688.6554335654657, 577.1034824791205, 555.9004748909855, 3026.8633478818015, 1514.5039056432947, 1692.989736772638, 1974.5289100882133, 2354.9099929808735, 2188.105669627681, 2650.1555815280535, 6265.39787520049, 2455.722175459128, 3213.8998296513846, 3159.95906562721, 5163.5551587700875, 3566.2097242718837, 2678.86798287456, 2221.0228742859526, 1953.8461880463203, 2661.9852198112185, 1678.1977169456454, 3670.5962410212455, 2023.4311795057438, 1783.7944233618816, 1902.0392576914765], \"Total\": [7138.0, 5030.0, 4922.0, 6191.0, 4150.0, 3258.0, 3559.0, 3511.0, 3088.0, 2905.0, 2890.0, 2815.0, 2821.0, 3049.0, 2389.0, 2654.0, 3672.0, 2433.0, 10286.0, 2302.0, 2520.0, 1970.0, 2025.0, 2161.0, 11105.0, 3157.0, 1707.0, 1720.0, 1678.0, 1625.0, 6434.56294656999, 5328.916228042143, 4188.671681298696, 4765.189073017618, 3657.0469098631997, 2817.7587787032207, 2641.642153266165, 3584.36845064899, 2621.498425465979, 2979.4558266265794, 2000.5331761322816, 2628.9836839660984, 2403.1639789271794, 1862.656024538106, 2178.5232318895437, 2236.7157523760543, 1701.7089240466732, 1788.444191398841, 1543.40817861641, 1558.3723841925773, 1865.033271069331, 1371.875688360797, 1979.8504488797962, 1800.3320911572803, 1229.7036562995145, 1267.7663892513033, 1488.8484571203503, 1353.3872515321136, 1728.6498031269907, 1431.1881099655752, 10286.882249856137, 3751.7246800963776, 4236.938982241289, 3728.195872016087, 3540.944097851995, 3921.1999300062644, 4394.55427015162, 10248.753935009683, 2485.7556718421865, 2515.09094208787, 2228.035485600134, 3544.8520905356086, 1943.465654351363, 6959.764756557666, 5334.086336910421, 2961.8569626153817, 3262.43166076973, 3405.3458858640597, 6149.996276307949, 3736.2865518875474, 4757.110331638349, 3146.4543719068074, 3799.932078016759, 7026.139028423949, 3909.9367013323417, 5424.744664869057, 11105.579079509174, 3376.4831315544243, 3280.920459656672, 4473.375211644598, 3920.1364569902703, 5030.423288810074, 4922.185796963005, 3258.5951824200447, 2815.980711806695, 2905.23924233841, 2890.35791780034, 1970.1072842976437, 2389.4321635496494, 1625.8510787224193, 1369.3344174615372, 1599.0425621359298, 1599.400018861727, 1440.5332026158817, 1353.1629961139113, 863.2034154341851, 1334.477218786151, 872.4534633023892, 731.3714361544231, 874.7343378500874, 953.6030646967881, 631.5332688624266, 887.4846961156898, 855.8003667228817, 537.6747907450484, 576.0083309840987, 513.8443327251189, 468.68421285364735, 520.4137428997011, 514.99082037318, 524.3979980067855, 1678.2749515145622, 1043.8281864812557, 1707.1139268734778, 577.8379986476046, 556.8063524250406, 3088.4551257817316, 1533.5031230892887, 1720.7158000887512, 2025.8953616245863, 2433.3608897863237, 2302.692067095182, 2821.1341398633376, 7138.4637904297115, 2654.902466812727, 3559.472890160717, 3511.3115034481825, 6191.452609807761, 4150.080670542124, 3049.0012791035138, 2520.8412418049916, 2161.4945181872813, 3672.78861919971, 1817.425425195906, 11105.579079509174, 3157.0859098860715, 2348.7077648024656, 4107.571150989746], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.3584, -5.547, -5.7878, -5.6588, -5.9236, -6.1843, -6.2488, -5.9437, -6.2565, -6.1286, -6.5269, -6.2537, -6.3435, -6.5983, -6.4417, -6.4153, -6.6887, -6.639, -6.7864, -6.7768, -6.5971, -6.9042, -6.5374, -6.6325, -7.0137, -6.9832, -6.8225, -6.9179, -6.6731, -6.862, -4.8901, -5.8986, -5.779, -5.9061, -5.9573, -5.8565, -5.744, -4.908, -6.3107, -6.2995, -6.4197, -5.9601, -6.5561, -5.298, -5.5615, -6.1398, -6.0453, -6.0052, -5.4318, -5.9157, -5.6828, -6.0846, -5.9224, -5.3797, -5.9327, -5.6842, -5.2139, -6.0718, -6.0904, -5.9858, -6.0807, -4.5809, -4.6026, -5.0151, -5.1611, -5.1299, -5.1351, -5.5184, -5.3255, -5.7106, -5.8823, -5.7272, -5.727, -5.8316, -5.8943, -6.3439, -5.9083, -6.3333, -6.5098, -6.3308, -6.2445, -6.6566, -6.3164, -6.3529, -6.8177, -6.7489, -6.8631, -6.9551, -6.8505, -6.8609, -6.8429, -5.6842, -6.1562, -5.6723, -6.746, -6.7834, -5.0887, -5.7812, -5.6698, -5.5159, -5.3398, -5.4132, -5.2216, -4.3612, -5.2978, -5.0288, -5.0457, -4.5546, -4.9248, -5.2109, -5.3983, -5.5265, -5.2172, -5.6785, -4.8959, -5.4915, -5.6175, -5.5533], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3068, 0.3068, 0.3068, 0.3068, 0.3067, 0.3067, 0.3067, 0.3067, 0.3067, 0.3067, 0.3067, 0.3067, 0.3066, 0.3066, 0.3066, 0.3066, 0.3066, 0.3066, 0.3066, 0.3066, 0.3066, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.306, 0.3061, 0.3041, 0.3049, 0.3053, 0.3041, 0.3026, 0.2918, 0.3057, 0.3051, 0.3062, 0.3014, 0.3064, 0.2889, 0.2913, 0.3013, 0.2992, 0.2964, 0.2787, 0.2931, 0.2846, 0.2961, 0.2696, 0.1976, 0.2308, 0.1518, -0.0943, 0.2383, 0.2484, 0.043, 0.0801, 1.3306, 1.3306, 1.3305, 1.3305, 1.3305, 1.3305, 1.3304, 1.3304, 1.3304, 1.3303, 1.3303, 1.3303, 1.3303, 1.3302, 1.3301, 1.3301, 1.3301, 1.33, 1.33, 1.33, 1.3299, 1.3299, 1.3298, 1.3297, 1.3297, 1.3297, 1.3297, 1.3296, 1.3296, 1.3296, 1.325, 1.3279, 1.3198, 1.3294, 1.3291, 1.3106, 1.3182, 1.3145, 1.305, 1.2979, 1.2797, 1.2682, 1.2003, 1.2527, 1.2286, 1.2253, 1.1492, 1.1791, 1.2013, 1.2041, 1.2297, 1.0088, 1.251, 0.2236, 0.8859, 1.0556, 0.5608]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.00019879042827756666, 0.9999158542361603, 0.1222952200402558, 0.8776398093381472, 0.9991555965636029, 0.9985401410269797, 0.16603534982591403, 0.8340530607986576, 0.07648181767073978, 0.9232841010899377, 0.14072015615149772, 0.8592604055415083, 0.09996264918544269, 0.8999486365413074, 0.12135121180034063, 0.878648368684088, 0.1190078911059039, 0.8810550871540419, 0.020074761482670702, 0.9801016614200679, 0.04994154522148986, 0.9501921821271287, 0.02517405437914749, 0.9748775960552214, 0.999492137461129, 0.8562614992888558, 0.14378556931007697, 0.9993616853420273, 0.0007289290192137325, 0.09622971432490025, 0.9040041432252649, 0.9997643482051298, 0.9997307155215146, 0.00035489198279074, 0.03205443151790332, 0.9677972592905425, 0.9997355342403487, 0.0006479167428647756, 0.9992458552853346, 0.0005145447246577418, 0.9997610428698849, 0.0006416951494671918, 0.9943086506782229, 0.005739642465714699, 0.9971821680011608, 0.0028322333765713444, 0.9998685637728085, 0.0006987201703513687, 0.9990864213728688, 0.0008976517712245003, 0.001730588854212496, 0.9985497688806102, 0.0007493571159720556, 0.9996423927067222, 0.001146192939867331, 0.9994802435643125, 0.24055781160476492, 0.7595666122175232, 0.010544111741251153, 0.9893891517207333, 0.9997137280737701, 0.0002734446739807905, 0.00536264930360662, 0.9944735208577165, 0.9997516325077532, 0.0005591452083376696, 0.09720540391147, 0.9029426825764874, 0.9338118619738106, 0.06634121696230054, 0.0017360860012068228, 0.9982494506939231, 0.0011267799933641256, 0.9994538541139795, 0.0003551160687313137, 0.999651733478648, 0.8963671191989949, 0.10361309348632396, 0.9957778948646413, 0.004323532907319055, 0.9998396433643403, 0.0002387391698577699, 0.9992737526527913, 0.0007996322373855358, 0.999733482984115, 0.0004998667414920575, 0.0028740362052427407, 0.997290563219231, 0.0006150624820975559, 0.9994765334085284, 0.9994301252647356, 0.0006716600304198492, 0.07495567256709967, 0.9250810644462151, 0.001921548025284815, 0.9992049731481037, 0.0007390092715155939, 0.9998795443605986, 0.9994277838437661, 0.0008132040552024134, 0.0006252344555502053, 0.9997498944247784, 0.9993954806991248, 0.0007887888561161206, 0.9721957105947056, 0.02780489488404326, 0.012389932380264033, 0.9879340819000005, 0.9996478015642911, 0.0005368677774244313, 0.9820734233236676, 0.017960377163929547, 0.999583404637153, 0.0005876445647484733, 0.0010486543479366485, 0.999367593583626, 0.9994459771386606, 0.0005361834641301827, 0.9779045840204112, 0.022072222984123643, 0.9896204711507327, 0.010571613341669547, 0.00030688070902300143, 0.9998173499969386, 0.926613466342162, 0.07340272283748289, 0.9996258310874663, 0.00038037512598457625, 0.9995704479192975, 0.0005050886548354206, 0.9922046916490107, 0.007662995764975368, 0.0004185094748680532, 0.9998191354597791, 0.9998155389447806, 0.0005554530771915447, 0.7971661278340435, 0.20279906292098066, 0.9849977923184925, 0.015026216940767492, 0.001795956521768713, 0.9985518261034043, 0.00034597791292264376, 0.9998761683464404, 0.0011432042355371452, 0.9991605018594648, 0.9999125120735216, 0.00015541071061136487, 0.9995156473143797, 0.0004161180879743462, 0.001941782184147216, 0.998076042651669, 0.3591919993209568, 0.6407807889120772, 0.27526767936356056, 0.7247898738534106, 0.00034420573198478, 0.9999176514157859, 0.9996799985088431, 0.00044708407804509983, 0.9997598226716683, 0.0004590265485177541, 0.998321324006329, 0.0016944633505057917, 0.9997569113343489, 0.000378552408683964, 0.999960322032406, 0.00020985526170669592, 0.0019461146816519433, 0.9983568316874469, 0.9862733890521225, 0.0136499166463116, 0.9983734416837929, 0.001987999684754665, 0.0011684968117381188, 0.9990647740360915, 0.016272297841721343, 0.9838928659297941, 0.9988914148428175, 0.0012068764577239034, 0.9998098700113122, 0.000381461224727704, 0.5370570390408088, 0.4630473654830546, 0.999847010107515, 0.0003356317590156143, 0.9997138649476155, 0.0007388868181431009, 0.9998972062570958, 0.0002789891758529843, 0.9998280648441569, 0.00018765541757585527, 0.9846109845762328, 0.01537282953831894, 0.943028042905913, 0.05669140787992884, 0.0006941874010151851, 0.9996298574618666, 0.0019069485463349544, 0.999241038279516, 0.9893675966810435, 0.010805813776792636, 0.6694833242616106, 0.33055457745317723, 0.0006253742231002561, 0.9993480085142091, 0.0005075865705234965, 0.999945543931288, 0.9634382733258564, 0.03657960119975254, 0.9943997407991686, 0.005641984344959822, 0.9980698782298164, 0.0018775837537244597, 0.0018598603044310744, 0.9987449834794869, 0.06061392033215537, 0.9393385314632264, 0.9996240978792724, 0.0005784861677542085, 0.768100111758071, 0.23203955646242075, 0.9990393347939532, 0.0009721118369115046, 0.00020316177431112036, 0.9999622531593344, 0.9971437493098582, 0.0028052637448614938, 0.0007302817976734954, 0.9997557810150152], \"Term\": [\"0\", \"0\", \"1\", \"1\", \"145\", \"1d9\", \"2\", \"2\", \"25\", \"25\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", \"6\", \"7\", \"7\", \"8\", \"8\", \"9\", \"9\", \"a86\", \"also\", \"also\", \"armenian\", \"armenian\", \"b\", \"b\", \"b8f\", \"believ\", \"believ\", \"c\", \"c\", \"car\", \"car\", \"christian\", \"christian\", \"claim\", \"claim\", \"come\", \"come\", \"could\", \"could\", \"cours\", \"cours\", \"day\", \"day\", \"directori\", \"directori\", \"disk\", \"disk\", \"do\", \"do\", \"drive\", \"drive\", \"e\", \"e\", \"even\", \"even\", \"f\", \"f\", \"fact\", \"fact\", \"file\", \"file\", \"first\", \"first\", \"font\", \"font\", \"ftp\", \"ftp\", \"g\", \"g\", \"get\", \"get\", \"go\", \"go\", \"god\", \"god\", \"good\", \"good\", \"govern\", \"govern\", \"graphic\", \"graphic\", \"h\", \"h\", \"happen\", \"happen\", \"imag\", \"imag\", \"interfac\", \"interfac\", \"j\", \"j\", \"jesu\", \"jesu\", \"k\", \"k\", \"kill\", \"kill\", \"know\", \"know\", \"l\", \"l\", \"law\", \"law\", \"like\", \"like\", \"live\", \"live\", \"mac\", \"mac\", \"made\", \"made\", \"make\", \"make\", \"mani\", \"mani\", \"max\", \"max\", \"may\", \"may\", \"mean\", \"mean\", \"might\", \"might\", \"much\", \"much\", \"n\", \"n\", \"never\", \"never\", \"new\", \"new\", \"one\", \"one\", \"output\", \"output\", \"p\", \"p\", \"pc\", \"pc\", \"peopl\", \"peopl\", \"person\", \"person\", \"printer\", \"printer\", \"program\", \"program\", \"q\", \"q\", \"r\", \"r\", \"realli\", \"realli\", \"reason\", \"reason\", \"right\", \"right\", \"said\", \"said\", \"say\", \"say\", \"scsi\", \"scsi\", \"see\", \"see\", \"seem\", \"seem\", \"server\", \"server\", \"softwar\", \"softwar\", \"someth\", \"someth\", \"state\", \"state\", \"system\", \"system\", \"take\", \"take\", \"talk\", \"talk\", \"thing\", \"thing\", \"think\", \"think\", \"time\", \"time\", \"two\", \"two\", \"u\", \"u\", \"unix\", \"unix\", \"us\", \"us\", \"use\", \"use\", \"v\", \"v\", \"w\", \"w\", \"want\", \"want\", \"way\", \"way\", \"well\", \"well\", \"widget\", \"widget\", \"window\", \"window\", \"word\", \"word\", \"work\", \"work\", \"would\", \"would\", \"x\", \"x\", \"year\", \"year\", \"z\", \"z\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1221620018028921602268070925\", ldavis_el1221620018028921602268070925_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1221620018028921602268070925\", ldavis_el1221620018028921602268070925_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1221620018028921602268070925\", ldavis_el1221620018028921602268070925_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "0      0.205578  0.0       1        1  73.571051\n",
       "1     -0.205578  0.0       2        1  26.428949, topic_info=         Term         Freq         Total Category  logprob  loglift\n",
       "192         1  7138.000000   7138.000000  Default  30.0000  30.0000\n",
       "2411        0  5030.000000   5030.000000  Default  29.0000  29.0000\n",
       "696         x  4922.000000   4922.000000  Default  28.0000  28.0000\n",
       "194         2  6191.000000   6191.000000  Default  27.0000  27.0000\n",
       "1025        3  4150.000000   4150.000000  Default  26.0000  26.0000\n",
       "...       ...          ...           ...      ...      ...      ...\n",
       "1700       25  1678.197717   1817.425425   Topic2  -5.6785   1.2510\n",
       "190       use  3670.596241  11105.579080   Topic2  -4.8959   0.2236\n",
       "218   program  2023.431180   3157.085910   Topic2  -5.4915   0.8859\n",
       "205     drive  1783.794423   2348.707765   Topic2  -5.6175   1.0556\n",
       "221    system  1902.039258   4107.571151   Topic2  -5.5533   0.5608\n",
       "\n",
       "[148 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "2411      1  0.000199     0\n",
       "2411      2  0.999916     0\n",
       "192       1  0.122295     1\n",
       "192       2  0.877640     1\n",
       "1441      2  0.999156   145\n",
       "...     ...       ...   ...\n",
       "696       2  0.999962     x\n",
       "161       1  0.997144  year\n",
       "161       2  0.002805  year\n",
       "1473      1  0.000730     z\n",
       "1473      2  0.999756     z\n",
       "\n",
       "[230 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the topic distributions and identify the main topic\n",
    "main_topic_doc1 = max(unseen_doc_topics[0], key=lambda x: x[1])\n",
    "main_topic_doc2 = max(unseen_doc_topics[1], key=lambda x: x[1])\n",
    "main_topic_doc3 = max(unseen_doc_topics[2], key=lambda x: x[1])\n",
    "\n",
    "print(f\"Main topic for unseen Document 1: {main_topic_doc1}\")\n",
    "print(f\"Main topic for unseen Document 2: {main_topic_doc2}\")\n",
    "print(f\"Main topic for unseen Document 3: {main_topic_doc3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "tokens_list=processed_docs\n",
    "dictionary = gensim.corpora.Dictionary(tokens_list)\n",
    "\n",
    "    # Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to perform LDA\n",
    "def perform_lda_sklearn(texts_list, num_topics=5, max_features=1000):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=max_features, stop_words='english')\n",
    "    data_vectorized = vectorizer.fit_transform(texts_list)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=5,\n",
    "                                          learning_method='online', learning_offset=50., random_state=0)\n",
    "    lda_model.fit(data_vectorized)\n",
    "\n",
    "    return lda_model, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHsOW1cymQzp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lda_gibbs_sampling(document_word_matrix, n_topics, n_iter, alpha, beta):\n",
    "    n_documents, n_words = document_word_matrix.shape\n",
    "\n",
    "    # Initialize topic assignments randomly\n",
    "    topic_assignments = np.random.randint(0, n_topics, size=document_word_matrix.nonzero()[0].shape)\n",
    "\n",
    "    # Initialize count matrices\n",
    "    doc_topic_counts = np.zeros((n_documents, n_topics))\n",
    "    topic_word_counts = np.zeros((n_topics, n_words))\n",
    "    topic_counts = np.zeros(n_topics)\n",
    "\n",
    "    # Update count matrices based on initial topic assignments\n",
    "    for d, w, z in zip(*document_word_matrix.nonzero(), topic_assignments):\n",
    "        doc_topic_counts[d, z] += 1\n",
    "        topic_word_counts[z, w] += 1\n",
    "        topic_counts[z] += 1\n",
    "\n",
    "    # Perform Gibbs sampling\n",
    "    for _ in range(n_iter):\n",
    "        for d, w, z in zip(*document_word_matrix.nonzero(), topic_assignments):\n",
    "            # Decrement count matrices\n",
    "            doc_topic_counts[d, z] -= 1\n",
    "            topic_word_counts[z, w] -= 1\n",
    "            topic_counts[z] -= 1\n",
    "\n",
    "            # Calculate conditional probability\n",
    "            p_z = (doc_topic_counts[d, :] + alpha) * (topic_word_counts[:, w] + beta) / (topic_counts + beta * n_words)\n",
    "            p_z /= np.sum(p_z)\n",
    "\n",
    "            # Sample a new topic assignment\n",
    "            new_z = np.random.choice(n_topics, p=p_z)\n",
    "\n",
    "            # Increment count matrices\n",
    "            doc_topic_counts[d, new_z] += 1\n",
    "            topic_word_counts[new_z, w] += 1\n",
    "            topic_counts[new_z] += 1\n",
    "\n",
    "            # Update topic assignment\n",
    "            z = new_z\n",
    "\n",
    "    return doc_topic_counts, topic_word_counts\n",
    "\n",
    "\n",
    "n_topics = 5\n",
    "n_iter = 1000\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "doc_topic_counts, topic_word_counts = lda_gibbs_sampling(document_word_matrix, n_topics, n_iter, alpha, beta)\n",
    "\n",
    "# Extract topics\n",
    "topics = np.argsort(-topic_word_counts, axis=1)[:, :5]\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic {i}: {[vectorizer.get_feature_names()[word] for word in topic]}\")\n",
    "\n",
    "\n",
    "def predict_topic(text, doc_topic_counts, topic_word_counts, alpha, beta):\n",
    "    words = preprocess(text)\n",
    "    word_indices = [vectorizer.vocabulary_.get(word) for word in words]\n",
    "    word_indices = [idx for idx in word_indices if idx is not None]\n",
    "\n",
    "    n_documents, n_topics = doc_topic_counts.shape\n",
    "    n_words = topic_word_counts.shape[1]\n",
    "\n",
    "    p_z = (doc_topic_counts.sum(axis=0) + alpha) * np.prod(topic_word_counts[:, word_indices] + beta, axis=1) / (topic_word_counts.sum(axis=1) + beta * n_words)**len(word_indices)\n",
    "    p_z /= np.sum(p_z)\n",
    "\n",
    "    return np.argmax(p_z)\n",
    "\n",
    "new_text = \"your new text here\"\n",
    "topic_id = predict_topic(new_text, doc_topic_counts, topic_word_counts, alpha, beta)\n",
    "print(f\"Topic ID: {topic_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHkR0XGpmdqT"
   },
   "outputs": [],
   "source": [
    "# Randomly assign topics to words in documents\n",
    "for doc in docs:\n",
    "    cur_topics = []\n",
    "    for word in doc:\n",
    "        topic = random.randint(0, K - 1)\n",
    "        cur_topics.append(topic)\n",
    "        doc_topic_counts[len(topic_assignments)][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "    topic_assignments.append(cur_topics)\n",
    "\n",
    "# Iterate until convergence or max_iters\n",
    "for _ in range(max_iters):\n",
    "    for d, doc in enumerate(docs):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = topic_assignments[d][i]\n",
    "\n",
    "            # Decrement counts\n",
    "            doc_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "\n",
    "            # Sample new topic\n",
    "            topic_probs = []\n",
    "            for k in range(K):\n",
    "                doc_topic_prob = (doc_topic_counts[d][k] + 1) / (sum(doc_topic_counts[d].values()) + K)\n",
    "                topic_word_prob = (topic_word_counts[k][word] + 1) / (topic_counts[k] + len(docs))\n",
    "                topic_probs.append(doc_topic_prob * topic_word_prob)\n",
    "\n",
    "            # Normalize and sample new topic\n",
    "            total_prob = sum(topic_probs)\n",
    "            topic_probs = [p / total_prob for p in topic_probs]\n",
    "            new_topic = random.choices(range(K), topic_probs)[0]\n",
    "\n",
    "            # Increment counts\n",
    "            topic_assignments[d][i] = new_topic\n",
    "            doc_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "\n",
    "return doc_topic_counts, topic_word_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def lda(docs, K, alpha, beta, num_iters):\n",
    "# Initialize topic assignments\n",
    "topic_assignments = []\n",
    "for d in docs:\n",
    "topic_assignments.append([random.randint(0, K-1) for _ in d])\n",
    "\n",
    "# Initialize topic-word and document-topic count matrices\n",
    "topic_word_counts = [[0 for _ in range(len(docs[0]))] for _ in range(K)]\n",
    "doc_topic_counts = [[0 for _ in range(K)] for _ in range(len(docs))]\n",
    "\n",
    "# Count initial topic assignments\n",
    "for d, doc in enumerate(docs):\n",
    "    for w, word in enumerate(doc):\n",
    "        topic = topic_assignments[d][w]\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        doc_topic_counts[d][topic] += 1\n",
    "\n",
    "# Gibbs sampling\n",
    "for _ in range(num_iters):\n",
    "    for d, doc in enumerate(docs):\n",
    "        for w, word in enumerate(doc):\n",
    "            old_topic = topic_assignments[d][w]\n",
    "\n",
    "            # Decrement counts for old topic assignment\n",
    "            topic_word_counts[old_topic][word] -= 1\n",
    "            doc_topic_counts[d][old_topic] -= 1\n",
    "\n",
    "            # Compute probabilities for each topic\n",
    "            probabilities = []\n",
    "            for t in range(K):\n",
    "                p_topic_given_doc = (doc_topic_counts[d][t] + alpha) / (sum(doc_topic_counts[d]) + K * alpha)\n",
    "                p_word_given_topic = (topic_word_counts[t][word] + beta) / (sum(topic_word_counts[t]) + len(docs[0]) * beta)\n",
    "                probabilities.append(p_topic_given_doc * p_word_given_topic)\n",
    "\n",
    "            # Normalize probabilities\n",
    "            total_prob = sum(probabilities)\n",
    "            probabilities = [p / total_prob for p in probabilities]\n",
    "\n",
    "            # Sample new topic assignment\n",
    "            new_topic = random.choices(range(K), probabilities)[0]\n",
    "\n",
    "            # Update counts for new topic assignment\n",
    "            topic_assignments[d][w] = new_topic\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            doc_topic_counts[d][new_topic] += 1\n",
    "\n",
    "return topic_word_counts, doc_topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Sentiment Analysis:**\n",
    "Determine the sentiment of the content using the VADER sentiment analyzer from the `vaderSentiment`library.\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a pre-trained sentiment analysis tool specifically designed for social media texts and doesn't require preprocessing like tokenization, stemming, or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T12:02:41.589294200Z",
     "start_time": "2023-06-19T12:02:41.567813800Z"
    },
    "id": "hmTOX4eUgcnL"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T12:08:49.380032600Z",
     "start_time": "2023-06-19T12:08:49.321967700Z"
    },
    "id": "xLWkj_argjag"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.26, 'neu': 0.52, 'pos': 0.22, 'compound': -0.1757}\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze sentiment using VADER\n",
    "def analyze_sentiment_vader(text):\n",
    "     # Initialize VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T13:27:55.925406600Z",
     "start_time": "2023-06-19T13:27:55.886404500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Think!\n",
      "\n",
      "It's the SCSI card doing the DMA transfers NOT the disks...\n",
      "\n",
      "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
      "it is attached when it wants to.\n",
      "\n",
      "An important feature of SCSI is the ability to detach a device. This frees the\n",
      "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
      "start transfers on several devices. While each device is seeking the data the\n",
      "bus is free for other commands and data transfers. When the devices are\n",
      "ready to transfer the data they can aquire the bus and send the data.\n",
      "\n",
      "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
      "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
      "processes wanting the bus irrespective of transfer time.\n",
      "\n",
      "The sentiment value of text 1 is: -0.5952\n",
      "The sentiment value of text 2 is: 0.8268\n",
      "The sentiment value of text 3 is: -0.9976\n",
      "The sentiment value of text 4 is: 0.8932\n",
      "The sentiment value of text 5 is: 0.2732\n",
      "The sentiment value of text 6 is: 0.3182\n",
      "The sentiment value of text 7 is: 0.2144\n",
      "The sentiment value of text 8 is: 0.8685\n",
      "The sentiment value of text 9 is: 0.937\n",
      "The sentiment value of text 10 is: 0.8011\n",
      "0.35396\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"This is an example text for sentiment analysis. Wow. So bad.\"\n",
    "sentiment = analyze_sentiment_vader(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on the texts\n",
    "import numpy as np\n",
    "print(texts[3])\n",
    "scores_list = []\n",
    "for i, text in enumerate(texts):\n",
    "    scores_list.append(analyzer.polarity_scores(text)[\"compound\"])\n",
    "    print(f'The sentiment value of text {i + 1} is: {scores_list[i]}')\n",
    "print(np.mean(scores_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Summarization:**\n",
    "Generate summaries of the relevant content using extractive summarization techniques. For this, you can use theÂ gensimÂ library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement extractive summarization without using libraries, you can follow these steps:\n",
    "\n",
    "1. Split the text into sentences.\n",
    "2. Tokenize the sentences.\n",
    "3. Calculate the frequency of each word in the text.\n",
    "4. Assign a score to each sentence based on the frequency of the words in the sentence.\n",
    "5. Select the top N sentences with the highest scores as the summary.\n",
    "\n",
    "This is a simple implementation of extractive summarization without using any libraries. Note that this approach does not consider the semantic meaning of words or the coherence of the summary. More advanced techniques, such as using word embeddings or graph-based methods, can improve the quality of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgY9tfBMk737"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes a list of texts and the number of sentences to include in the summary (default is 3). It calculates the frequency of words in the text, scores each sentence based on the frequency of the words it contains, and selects the top N sentences with the highest scores as the summary.\n",
    "\"\"\"\n",
    "def extractive_summarization(texts, n_sentences=3):\n",
    "    summaries = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Split the text into sentences\n",
    "        sentences = text.strip().split('.')\n",
    "\n",
    "        # Tokenize and preprocess the text\n",
    "        word_freq = {}\n",
    "        for sentence in sentences:\n",
    "            stemmed_tokens, _ = preprocess_text(sentence)\n",
    "            for token in stemmed_tokens:\n",
    "                if token not in word_freq:\n",
    "                    word_freq[token] = 1\n",
    "                else:\n",
    "                    word_freq[token] += 1\n",
    "\n",
    "        # Calculate the score for each sentence\n",
    "        sentence_scores = {}\n",
    "        for sentence in sentences:\n",
    "            stemmed_tokens, _ = preprocess_text(sentence)\n",
    "            for token in stemmed_tokens:\n",
    "                if token in word_freq:\n",
    "                    if sentence not in sentence_scores:\n",
    "                        sentence_scores[sentence] = word_freq[token]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_freq[token]\n",
    "\n",
    "        # Select the top N sentences with the highest scores\n",
    "        summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n_sentences]\n",
    "        summary = '. '.join(summary_sentences)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T15:05:32.674379100Z",
     "start_time": "2023-06-19T15:05:32.632503500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1:  Some sentences are more important than others.  It has several sentences\n",
      "Summary 2:  Extractive summarization should work on it as well. Another example text is here\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "\n",
    "# Function to generate extractive summary\n",
    "def generate_summary(text, word_count=50):\n",
    "    summary = summarize(text, word_count=word_count)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "long_text = \"This is an example long text that needs summarization. ...\"\n",
    "summary = generate_summary(long_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "texts = [\n",
    "    \"This is an example text. It has several sentences. Some sentences are more important than others.\",\n",
    "    \"Another example text is here. Extractive summarization should work on it as well.\"\n",
    "]\n",
    "\n",
    "summaries = extractive_summarization(texts, n_sentences=2)\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Summary {i + 1}: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S5Wh1_zk8iJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Visualization and Reporting:**\n",
    " Visualize the results in an intuitive dashboard or report format, showing the distribution of topics, sentiment scores, and summaries of the relevant content. You can use the matplotlib library for basic visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T16:02:11.692891800Z",
     "start_time": "2023-06-19T16:02:11.668955100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T16:01:29.723467100Z",
     "start_time": "2023-06-19T16:01:29.698198300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize_sentiment(scores):\n",
    "    plt.hist(scores, bins=[-1, -0.5, 0, 0.5, 1], edgecolor='black')\n",
    "    plt.xlabel('Sentiment Scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Sentiment Analysis of Texts')\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize sentiment scores\n",
    "def visualize_sentiment(sentiment_scores):\n",
    "    labels = ['Positive', 'Neutral', 'Negative']\n",
    "    values = [sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg']]\n",
    "\n",
    "    plt.bar(labels, values)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Sentiment Analysis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T16:02:38.275615600Z",
     "start_time": "2023-06-19T16:02:38.211301700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m sia \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m      9\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m [sia\u001b[38;5;241m.\u001b[39mpolarity_scores(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m---> 10\u001b[0m \u001b[43mvisualize_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m     13\u001b[0m generate_wordcloud(texts)\n",
      "Cell \u001b[1;32mIn[54], line 11\u001b[0m, in \u001b[0;36mvisualize_sentiment\u001b[1;34m(sentiment_scores)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_sentiment\u001b[39m(sentiment_scores):\n\u001b[0;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\u001b[43msentiment_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, sentiment_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m], sentiment_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mbar(labels, values)\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_wordcloud(texts):\n",
    "    all_text = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10, max_words=100).generate(all_text)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Texts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"This is an example text. It has several sentences. Some sentences are more important than others.\",\n",
    "    \"Another example HPC text is here for computer topic. Extractive computing summarization should work on if computer is it as well.\"\n",
    "]\n",
    "\n",
    "# Analyze sentiment using the code from the previous answer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = [sia.polarity_scores(text)[\"compound\"] for text in texts]\n",
    "visualize_sentiment(sentiment_scores)\n",
    "\n",
    "# Generate word cloud\n",
    "generate_wordcloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMjXohX+9xAkasGPVtSJXRB",
   "collapsed_sections": [
    "xSaT22-QkIgK",
    "va708914k1Zr",
    "3S5Wh1_zk8iJ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
